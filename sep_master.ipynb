{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEP Forecasting \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current physics based models of SEPs are unable to execute sufficiently fast in order to\n",
    "provide actionable information towards forecasting such disturbances, which can impact\n",
    "Earth within tens of minutes of the onset of an eruptive event. This is compounded by\n",
    "the intrinsic latency of certain key observations, which are used to define the initial\n",
    "conditions of these models. Instead, there is a reliance on statistical models to provide\n",
    "forecast probabilities of Earth-bound SEPs using real-time data. Since the largest, most\n",
    "impactful events occur infrequently, some regions of the feature space are sparse and\n",
    "simple discrete binning procedures have limitations. The goal of this project is to\n",
    "improve upon the empirical SEP proton prediction forecast model (PROTONS) currently\n",
    "in operational use at SWPC, through the application of modern machine learning\n",
    "techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test comment by Hazel\n",
    "# ... and another \n",
    "# a third from Eric\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "from datetime import datetime,timedelta\n",
    "\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, make_scorer, recall_score, matthews_corrcoef\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, GridSearchCV\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, brier_score_loss, precision_score\n",
    "from sklearn.metrics import brier_score_loss as bsl\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn import preprocessing\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "#import scikitplot as skplt\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier as trees\n",
    "\n",
    "\n",
    "\n",
    "from scipy.sparse import csr_matrix, find\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in and format the original Balch 2008 event list \n",
    "\n",
    "Choice of files: \n",
    "\n",
    "SPEall.v7p.xls - original SEP (only) event list from Balch 2008  \n",
    "ctrlevents.v8p.xls - original event list (SEP + control) from Balch 2008  \n",
    "ControlEvents_student.xls - Balch 2008 (SEP + control) event list with added CME speed and GOES T and EM from Dougs student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read in the original Balch SEP event list with added CME speed and GOES T and EM\n",
    "#orig_data = pd.read_excel(\"ctrlevents.v8p.xls\")\n",
    "\n",
    "#adding y label column indicating positive and negative SEP events - SEP events have Association = ProtonFlare\n",
    "#orig_data['sep'] = orig_data.Association.str.contains('^Proton').astype(int)\n",
    "\n",
    "#convert the type II and type IV association to binary \n",
    "#orig_data.TypeII = (orig_data.TypeII.str.lower() == \"yes\").astype(int)\n",
    "#orig_data.TypeIV = (orig_data.TypeIV.str.lower() == \"yes\").astype(int)\n",
    "\n",
    "# Remove rows where optlocation = nan\n",
    "#orig_data = orig_data[orig_data.optlocation.astype('str') != 'nan']\n",
    "\n",
    "#shuffle the events so they are not organized \n",
    "#orig_data = shuffle(orig_data)\n",
    "\n",
    "#save the shuffled dataframe -- commented out to prevent resaving\n",
    "#orig_data.to_csv(\"AllEvtsShuffled_1986_2004.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read in the students version Balch SEP event list with added CME speed and GOES T and EM\n",
    "#orig_data = pd.read_excel(\"ControlEvents_student.xls\")\n",
    "\n",
    "#adding y label column indicating positive and negative SEP events - SEP events have Association = ProtonFlare\n",
    "#orig_data['sep'] = orig_data.Association.str.contains('^Proton').astype(int)\n",
    "\n",
    "#convert the type II and type IV association to binary \n",
    "#orig_data.TypeII = (orig_data.TypeII.str.lower() == \"yes\").astype(int)\n",
    "#orig_data.TypeIV = (orig_data.TypeIV.str.lower() == \"yes\").astype(int)\n",
    "\n",
    "# Remove rows where optlocation = nan\n",
    "#orig_data = orig_data[orig_data.optlocation.astype('str') != 'nan']\n",
    "\n",
    "#shuffle the events so they are not organized \n",
    "#orig_data = shuffle(orig_data)\n",
    "\n",
    "#save the shuffled dataframe -- commented out to prevent resaving\n",
    "#orig_data.to_csv(\"AllEvtsShuffled_1986_2004_cme.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the new SEP events from 2004 onwards\n",
    "\n",
    "Created by Pedro Brae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read in the SEP event list from 2004 - 2017\n",
    "#new_data = pd.read_csv('sep_events_2004_2017.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Balch Features\n",
    "\n",
    "From the original Balch 1999 and 2008 paper analysis only 4 features were used. GOES flare peak soft X-ray peak (1-8 A channel), flare soft X-ray integrated flux, occurrence of Type II and Type IV radio bursts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BalchPaperFeatures(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        \"\"\"Class to create original 4 features from Balch 2008\"\"\"\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def fit(self, examples):\n",
    "        # return self and nothing else \n",
    "        return self\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        \"\"\"Array mapping from feature integer indices to feature name\"\"\"\n",
    "        \n",
    "        return ['FlrPeakFlux','FlrIntFlux2','TypeII','TypeIV']\n",
    "    \n",
    "    def transform(self, examples):\n",
    "                \n",
    "        #Choose the orginal 4 Balch 2008 features\n",
    "        X = examples[['FlrPeakFlux','FlrIntFlux2','TypeII','TypeIV']]\n",
    "        \n",
    "        return(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw features\n",
    "\n",
    "Other features read directly from the original event list i.e. the flare integrated soft X-ray flux at the GOES short wavelength channel (0.5 - 1 A), the GOES temperature and GOES emission measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RawFeatures(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        \"\"\"Other features from the Event List that don't need to be manipulated before being included in model\"\"\"\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def fit(self, examples):\n",
    "        # return self and nothing else \n",
    "        return self\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        \"\"\"Array mapping from feature integer indices to feature name\"\"\"\n",
    "        \n",
    "        return ['FlrIntFlux','tchianti','emchianti']\n",
    "    \n",
    "    def transform(self, examples):\n",
    "                \n",
    "        #Choose the orginal 4 Balch 2008 features (5 if you include both type II and type Iv as separate features)\n",
    "        X = examples[['FlrIntFlux','tchianti','emchianti']]\n",
    "\n",
    "        return(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flare time to peak\n",
    "\n",
    "The time between the flare onset and the flare peak time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FlareTime2Peak(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    \"\"\"Class to create feature with the time between flare onset to flare max time\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def fit(self, examples):\n",
    "        # return self and nothing else \n",
    "        return self\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        \"\"\"Array mapping from feature integer indices to feature name\"\"\"\n",
    "        \n",
    "        return [\"FlTim2Pk\"]\n",
    "    \n",
    "    def transform(self, examples):\n",
    "        \n",
    "        # Initiaize matrix \n",
    "        X = np.zeros((len(examples),1))           \n",
    "\n",
    "        #time between flare max and flare onset\n",
    "        X[:,0] = np.asarray([(mx - on).seconds for mx, on in zip(examples.Flrmaxtime, examples.FlrOnset)])\n",
    "        \n",
    "        return(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flare location\n",
    "\n",
    "The flare location formatted from N16W12 format to North-South and East-West of disk center values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LocationFeatures(BaseEstimator, TransformerMixin):\n",
    "   \n",
    "    \"\"\"Class to create feature with the time between flare onset to flare max time\"\"\"\n",
    "   \n",
    "    def __init__(self):\n",
    "       \n",
    "        return None\n",
    "   \n",
    "    def fit(self, examples):\n",
    "        # return self and nothing else \n",
    "        return self\n",
    "   \n",
    "    def get_feature_names(self):\n",
    "        \"\"\"Array mapping from feature integer indices to feature name\"\"\"\n",
    "       \n",
    "        return ['NS', 'EW']\n",
    "   \n",
    "    def transform(self, examples):\n",
    "       \n",
    "        # Initiaize matrix \n",
    "        X = np.zeros((len(examples),2))  \n",
    "       \n",
    "        #time between flare max and flare onset\n",
    "        #X[:,0] = np.asarray([str(x)[1:3] for x in examples.optlocation])   #north - south\n",
    "        #X[:,1] = np.asarray([str(x)[4::] for x in examples.optlocation])   #north - south\n",
    "       \n",
    "        for i,loc in enumerate(examples.optlocation):\n",
    "            lat = str(loc)[1:3]\n",
    "            if str(loc)[0] == 's' or str(loc)[0] == 'S':\n",
    "                X[i,0] = -int(lat)\n",
    "            else:\n",
    "                X[i,0] = (lat)\n",
    "\n",
    "            #west-east\n",
    "            long = str(loc)[4::]\n",
    "            if str(loc)[3] == 'w' or str(loc)[3] == 'W':\n",
    "                X[i,1] = -int(long)\n",
    "            else:\n",
    "                X[i,1] = int(long)\n",
    "\n",
    "        return(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SEP Class \n",
    "\n",
    "With functions to build the training feature list from the above feature extractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SEPClass(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, estimator, folds = 6, threshold=0.5):        # <--- other keywords to be used by Feature Union go here\n",
    "        \n",
    "        \"\"\"Class to fit and train Logistic Regression algorithm for SEP forecasting\n",
    "        \n",
    "        Input keywords:\n",
    "        \n",
    "        folds:        Number of cross validation folds to use\n",
    "        threshold:    Decision Boundary threshold\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.estimator = estimator     #estimator to use for classification e.g. LogReg or SVM\n",
    "        self.folds = folds             #cross validation folds for estimator\n",
    "        self.threshold = threshold     #decision boundary threshold\n",
    "    \n",
    "        #Set up the Feature union to combine Feature creating classes\n",
    "        self.allmyfeatures = FeatureUnion([\n",
    "            (\"BalchFeat\", BalchPaperFeatures()),\n",
    "            (\"RawFeat\", RawFeatures()),\n",
    "            (\"LocFeatures\", LocationFeatures())#,\n",
    "            #(\"FlareTime2Peak\", FlareTime2Peak())   \n",
    "        ])\n",
    "    \n",
    "    def build_train_features(self, examples):\n",
    "        \"\"\"\n",
    "        Method to take in training text features and do further feature engineering \n",
    "        Most of the work in this homework will go here, or in similar functions  \n",
    "        :param examples: currently just a list of forum posts  \n",
    "        \"\"\"  \n",
    "        \n",
    "        ##convert columns of time from string to datetime -- MOVE TO SEPARATE CLEANING FUNCTION\n",
    "        #examples.FlrOnset = pd.Series(datetime.strptime(t, \"%Y-%m-%dT%H:%M:%S.%f\") for t in examples.FlrOnset)\n",
    "        #examples.Flrmaxtime = pd.Series(datetime.strptime(t, \"%Y-%m-%dT%H:%M:%S.%f\") for t in examples.Flrmaxtime)\n",
    "        #examples.Flrendtime = pd.Series(datetime.strptime(t, \"%Y-%m-%dT%H:%M:%S.%f\") for t in examples.Flrendtime)\n",
    "        #examples.FlrHpTime = pd.Series(datetime.strptime(t, \"%Y-%m-%dT%H:%M:%S.%f\") for t in examples.FlrHpTime)\n",
    "                \n",
    "        return self.allmyfeatures.fit_transform(examples)\n",
    "        #normalize data here?\n",
    "\n",
    "    def get_test_features(self, examples):\n",
    "        \"\"\"\n",
    "        Method to take in test text features and transform the same way as train features \n",
    "        :param examples: currently just a list of forum posts  \n",
    "        \"\"\"\n",
    "    \n",
    "        return self.allmyfeatures.transform(examples)\n",
    "\n",
    "    def show_topX(self, num=3):\n",
    "        \"\"\"\n",
    "        prints the top num features for the positive class and the \n",
    "        top 10 features for the negative class. \n",
    "        \"\"\"\n",
    "        feature_names = np.asarray([x.split(\"__\")[1] for x in self.allmyfeatures.get_feature_names()])\n",
    "        topX = np.argsort(self.estimator.coef_[0])[-num:]\n",
    "        bottomX = np.argsort(self.estimator.coef_[0])[:num]\n",
    "        \n",
    "        print(feature_names)\n",
    "        \n",
    "        print(\"\\nTop 3 features for Pos and Neg\\n-------------------------\")\n",
    "        for fn in np.arange(1,num):\n",
    "            print(\"Pos %i: %s %f\" % (fn, feature_names[topX[-fn]], self.estimator.coef_[0,topX[-fn]]))\n",
    "        for fn in np.arange(0,num-1):\n",
    "            print(\"Neg %i: %s %f\" % (fn, feature_names[bottomX[fn-1]], self.estimator.coef_[0,bottomX[fn-1]]))\n",
    "            \n",
    "      \n",
    "    def show_misclassified(self):     \n",
    "\n",
    "        \"\"\"\n",
    "        Method to show the misclassified examples i.e. False Positives and False negatives \n",
    "        \"\"\"\n",
    "        \n",
    "        #get all the feature names\n",
    "        #words = feat.allmyfeatures.get_feature_names() #####\n",
    "        words = self.allmyfeatures.get_feature_names() #####\n",
    "        \n",
    "        # False positives\n",
    "        print(\"\\nSome misclassified examples:\")\n",
    "        falsepos = np.where((self.train_pred != self.y_train) & (self.train_pred == 1))[0]   #all false pos example rows\n",
    "        print(\"\\nPredicted SEP but labeled AllClear (False Pos) \\n------------------------- \")\n",
    "\n",
    "        for i in range(len(falsepos[0:10])):         #loop through falsepos examples\n",
    "            weights_falsepos = []\n",
    "            x = find(feat.X_train[falsepos[i]])      #find which features are used for this example\n",
    "            for ii in x[1]:                          #from sparse matrix get column indices corresponding to features\n",
    "                weights_falsepos.append((words[ii].split('__')[1], self.estimator.coef_[0,ii]))      #get the word and weight\n",
    "\n",
    "            print(\"label: %i, prediction %i, Neg Prob: %f, Pos Prob: %f, Ex No.: %i,  example: %s \" % \\\n",
    "                (self.y_train[falsepos[i]], self.train_pred[falsepos[i]] , self.train_pred_prob[falsepos[i]][0], \\\n",
    "                     self.train_pred_prob[falsepos[i]][1], falsepos[i], self.clean_examples[falsepos[i]]))\n",
    "            for j in weights_falsepos:\n",
    "                print(j)\n",
    "                \n",
    "        # False Negatives\n",
    "        falseneg = np.where((self.train_pred != self.y_train) & (self.train_pred == 0))[0]\n",
    "        print(\"\\nPredicted AllClear but labeled SEP (False Neg) \\n-------------------------\")\n",
    "\n",
    "        for i in range(len(falseneg[0:10])):\n",
    "            weights_falseneg = []\n",
    "            x = find(feat.X_train[falseneg[r]])\n",
    "            for ii in x[1]:\n",
    "                weights_falseneg.append((words[ii].split('__')[1], self.estimator.coef_[0,ii])) \n",
    "\n",
    "            print(\"label: %i, prediction %i, Neg Prob: %f, Pos Prob: %f, Ex No.: %i, \\nexample: %s \" % \\\n",
    "                (self.y_train[falseneg[i]], self.train_pred[falseneg[i]], self.train_pred_prob[falseneg[i]][0], \\\n",
    "                 self.train_pred_prob[falseneg[i]][1], falseneg[i], self.clean_examples[falseneg[i]]))\n",
    "            for j in weights_falseneg:\n",
    "                print(j)\n",
    "        \n",
    "        print(\"\\n\\n\")\n",
    "        print(words)\n",
    "       \n",
    "    def show_report(self, X, y, y_pred, y_pred_prob):\n",
    "        \n",
    "        \"\"\"Method to show a report card of the model fit. Generalized to work for both the training and test sets\n",
    "            \n",
    "            X : features \n",
    "            y : labels\n",
    "            y_pred : predicted labels \n",
    "            y_pred_prob : predicted probabilities\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        #print the model parameters\n",
    "        print(\"Model Parameters \\n-----------------------------------\\n\")\n",
    "        print(self,'\\n')\n",
    "        \n",
    "        #print the shape of the training features\n",
    "        print(\"Features \\n-----------------------------------\\n\")\n",
    "        \n",
    "        print(\"Shape of the Features: Num examples x Num Features\")\n",
    "        print(self.X_train.shape,'\\n')\n",
    "\n",
    "        #print the names of the features\n",
    "        print(\"Feature Names: \\n\")\n",
    "        names = self.allmyfeatures.get_feature_names()\n",
    "        for n in names:\n",
    "            print(n)\n",
    "        print('\\n')\n",
    "        \n",
    "        #print(\"\\nTRAINING SET\\n-------------------------------------------------------------\\n\")\n",
    "        \n",
    "        # Add confusion Matrix\n",
    "        tn, fp, fn, tp=confusion_matrix(y, y_pred).ravel()\n",
    "        print(\"True Pos: %i, True Neg: %i, False Pos: %i,. False Neg: %i\\n\" % (tp,tn,fp,fn))\n",
    "\n",
    "        print(\"\\nCross Validation Metric Scores\\n-----------------------------------\\n\")\n",
    "        \n",
    "        #recall\n",
    "        metric1 = 'recall'\n",
    "        rec_score = cross_val_score(self.estimator, X, y, \\\n",
    "                                    cv=self.folds,scoring=metric1)\n",
    "        print(metric1)\n",
    "        print(rec_score)\n",
    "        print(\"mean: %.4f, stddev: %.4f \\n\" % (rec_score.mean(), rec_score.std()))\n",
    "        \n",
    "        #precision\n",
    "        metric2 = 'precision'\n",
    "        prec_score = cross_val_score(self.estimator, X, y, \\\n",
    "                                     cv=self.folds,scoring=metric2)\n",
    "        print(metric2)\n",
    "        print(prec_score)\n",
    "        print(\"mean: %.4f, stddev: %.4f \\n\" % (prec_score.mean(), prec_score.std()))\n",
    "    \n",
    "        #matthews correlation\n",
    "        metric3s = 'matthews correlation coefficient' \n",
    "        metric3 = make_scorer(matthews_corrcoef)\n",
    "        mattco_score = cross_val_score(self.estimator, X, y, \\\n",
    "                                       cv=self.folds,scoring=metric3)\n",
    "        print(metric3s)\n",
    "        print(mattco_score) \n",
    "        print(\"mean: %.4f, stddev: %.4f \\n\" % (mattco_score.mean(), mattco_score.std()))\n",
    "    \n",
    "        #brier\n",
    "        metric4s = \"brier score\" \n",
    "        metric4 = make_scorer(brier_score_loss)\n",
    "        brier_score = cross_val_score(self.estimator, X, y, cv=self.folds,scoring=metric4)\n",
    "        print(metric4s)\n",
    "        print(brier_score)  \n",
    "        print(\"mean: %.4f, stddev: %.4f \\n\" % (brier_score.mean(), brier_score.std()))\n",
    "        \n",
    "        #TSS\n",
    "        def tss_score(y, y_pred):\n",
    "            tn, fp, fn, tp=confusion_matrix(y, y_pred).ravel()\n",
    "            tss = tp / (tp + fp + fn)\n",
    "            return tss\n",
    "        \n",
    "        metric5s = \"TSS\" \n",
    "        metric5 = make_scorer(tss_score)\n",
    "        tss_score = cross_val_score(self.estimator, X, y, cv=self.folds,scoring=metric5)\n",
    "        print(metric5s)\n",
    "        print(tss_score)\n",
    "        print(\"mean: %.4f, stddev: %.4f \\n\" % (tss_score.mean(), tss_score.std()))\n",
    "        \n",
    "        #HSS\n",
    "        def hss_score(y, y_pred):\n",
    "            tn, fp, fn, tp=confusion_matrix(y, y_pred).ravel()\n",
    "            E = ((tp + fn)*(tp + fp) + (fp + tn)*(fn + tn)) / X_train.shape[0]\n",
    "            HSS = (tp + tn - E)/(X_train.shape[0] - E)\n",
    "            return HSS\n",
    "        \n",
    "        metric6s = \"HSS\" \n",
    "        metric6 = make_scorer(hss_score)\n",
    "        hss_score = cross_val_score(self.estimator, X, y, cv=self.folds,scoring=metric6)\n",
    "        print(metric6s)\n",
    "        print(hss_score)\n",
    "        print(\"mean: %.4f, stddev: %.4f \\n\" % (hss_score.mean(), hss_score.std()))\n",
    "        \n",
    "        \n",
    "        #cross validation\n",
    "        #print(\"\\nCross Validation Accuracy Scores (cross_val_predict)\\n-------------------------\")\n",
    "        #self.y_pred = cross_val_predict(self.estimator, self.X_train, self.y_train, cv=self.folds)\n",
    "        #print(self.score(self.y_pred, self.y_train))\n",
    "                \n",
    "        #training set score (no cross-validation)\n",
    "        print('\\nFull dataset (no cross-validation) Score \\n-----------------------------------\\n')\n",
    "        \n",
    "        rec1_score = recall_score(y,y_pred)\n",
    "        print(\"Recall: %.4f\" % rec1_score)\n",
    "        \n",
    "        prec1_score = precision_score(y,y_pred)\n",
    "        print(\"Precision: %.4f\" % prec1_score,'\\n')        \n",
    "        \n",
    "        mattco1_score = matthews_corrcoef(y,y_pred)\n",
    "        print(\"Mathews Corr: %.4f\" % mattco1_score,'\\n')        \n",
    "\n",
    "        #calculate the mean square error\n",
    "        mserr = mse(y, y_pred)\n",
    "        print(\"MSE: %.4f\" % mserr)\n",
    "\n",
    "        #calculate the Brier score - is this the same as the MSE? And QR referenced in Balch paper?\n",
    "        bsloss = bsl(y, y_pred_prob[:,1])\n",
    "        print(\"BSL: %.4f\" % bsloss)\n",
    "\n",
    "        #Occurance rate = #SEPS / #events\n",
    "        occ_rate = y.sum()/len(y)\n",
    "\n",
    "        #Reference score of predicting all negative class\n",
    "        #QR_star = 0.0324 Balch\n",
    "        QR_star =  mse(y, np.zeros(len(y)))\n",
    "        print(\"RefQuadScore: %.4f\\n\" % QR_star)\n",
    "\n",
    "        #assume the QR is the same as the MSE - as stated later in Balch\n",
    "        QR = mserr\n",
    "        #QR = 0.0250 Balch\n",
    "\n",
    "        #skill score \n",
    "        SS = (QR_star - QR)/QR_star\n",
    "        print(\"SS: %.4f\\n\" % SS)\n",
    "\n",
    "        FAR = fp/(tp + fp)\n",
    "        POD = tp/(tp + fn)\n",
    "        TSS = tp / (tp+fp+fn)\n",
    "        E = ((tp + fn)*(tp + fp) + (fp + tn)*(fn + tn)) / X.shape[0]\n",
    "        HSS = (tp + tn - E)/(X.shape[0] - E)\n",
    "\n",
    "        print(\"FAR: %.4f\" % FAR)\n",
    "        print(\"POD: %.4f\" % POD)\n",
    "        print(\"TSS: %.4f\" % TSS)\n",
    "        print(\"HSS: %.4f\" % HSS)\n",
    "        \n",
    "        ##ROC metrics\n",
    "        #fpr, tpr, thresh = roc_curve(self.y_train, self.y_train_pred, drop_intermediate=False)\n",
    "\n",
    "    def roc_curve(self):\n",
    "        \n",
    "        \"\"\"plot and ROC curve\"\"\"\n",
    "        \n",
    "        # Initial implementation of ROC plot applied to training set\n",
    "        skplt.metrics.plot_roc_curve(self.y_train, self.y_train_probas)\n",
    "        \n",
    "    def score(self, X, y):\n",
    "\n",
    "        \"\"\"find the accuracy score given the training data and labels\"\"\"\n",
    "        #print(\"...In Score...\")\n",
    "        #print(\"threshold:\", self.threshold)\n",
    "        \n",
    "        y_pred = (self.estimator.predict(X) > self.threshold).astype(int)\n",
    "        \n",
    "        #tn, fp, fn, tp=confusion_matrix(y, y_pred).ravel()\n",
    "        #return tp/(tp+fp+fn)\n",
    "        \n",
    "        return accuracy_score(y, y_pred)\n",
    "        \n",
    "    def fit(self, X, y):    #, random_state=1234):\n",
    "        \"\"\"\n",
    "        Method to read in training data from file, and \n",
    "        train Logistic Regression classifier. \n",
    "        \n",
    "        :param random_state: seed for random number generator \n",
    "        \"\"\"\n",
    "        \n",
    "        from sklearn.linear_model import LogisticRegression \n",
    "        \n",
    "        # load data \n",
    "        #self.dfTrain = pd.read_csv(\"AllEvtsShuffled.csv\")\n",
    "                      \n",
    "        # get training features and labels \n",
    "        #self.X_train = self.build_train_features(self.dfTrain)    #CHANGE\n",
    "        #self.y_train = np.array(self.dfTrain.sep, dtype=int)\n",
    "        \n",
    "        self.X_train = X \n",
    "        self.y_train = y \n",
    "        \n",
    "        #print the shape of the features\n",
    "        #print(\"Shape of the Features: Num examples x Num Features\")\n",
    "        #print(self.X_train.shape)\n",
    "        #print(\"examples...:\", self.X_train[0:10])\n",
    "\n",
    "        #self.logreg.fit(self.X_train, self.y_train)\n",
    "        self.estimator.fit(self.X_train, self.y_train)\n",
    "        \n",
    "        # make predictions on training data \n",
    "        self.y_train_pred = self.estimator.predict(self.X_train)\n",
    "\n",
    "        #return the LogReg probabilities used to classify each example  \n",
    "        self.y_train_pred_prob = self.estimator.predict_proba(self.X_train)\n",
    "        \n",
    "\n",
    "        #print report card\n",
    "        print(\"\\nTraining Set Report Card:\\n----------------------------------------\\n\")\n",
    "        print(self.show_report(self.X_train, self.y_train, self.y_train_pred, self.y_train_pred_prob))\n",
    "        \n",
    "    \n",
    "    def predict(self, X, y):\n",
    "        \n",
    "        \"\"\"\n",
    "        Return predicted labels for exmaples X. \n",
    " \n",
    "        #### CURRENTLY THIS FUNCTION ISN'T USED - but could be called if we need predicted y vals \n",
    "        #### independently to the score function\n",
    " \n",
    "        \"\"\"\n",
    "        \n",
    "        self.X_test = X\n",
    "        self.y_test = y\n",
    "    \n",
    "        self.y_test_pred = self.estimator.predict(self.X_test)\n",
    "        self.y_test_pred_prob = self.estimator.predict_proba(self.X_test)\n",
    "        \n",
    "        print(\"\\nTest Set Report Card:\\n----------------------------------------\\n\")\n",
    "        print(self.show_report(self.X_test, self.y_test, self.y_test_pred, self.y_test_pred_prob))\n",
    "        \n",
    "    def model_predict(self):\n",
    "        \"\"\"\n",
    "        Method to read in test data from file, make predictions\n",
    "        using trained model, and dump results to file \n",
    "        \n",
    "        #### CURRENTLY THIS FUNCTION ISN'T USED - leftover from FeatEngr homework but we might need to\n",
    "        #### to test on the holy grail test set\n",
    "        \"\"\"\n",
    "        \n",
    "        # read in test data \n",
    "        dfTest  = pd.read_csv(\"../data/spoilers/test.csv\")\n",
    "        \n",
    "        # featurize test data \n",
    "        self.X_test = self.get_test_features(list(dfTest[\"sentence\"]))\n",
    "        \n",
    "        # make predictions on test data \n",
    "        pred = self.estimator.predict(self.X_test)\n",
    "        \n",
    "        # dump predictions to file for submission to Kaggle  \n",
    "        #pd.DataFrame({\"spoiler\": np.array(pred, dtype=bool)}).to_csv(\"prediction.csv\", index=True, index_label=\"Id\")\n",
    "        \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in data\n",
    "\n",
    "Read in the shuffled data split into training and test sets - note: neither the training or the test data is featuized at this point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read in the shuffled dataframe of original sep+ctrl events from 1986 - 2004\n",
    "orig_data = pd.read_csv(\"AllEvtsShuffled_1986_2004_cme.csv\")\n",
    "\n",
    "feature_data_old = orig_data[['FlrOnset','Flrmaxtime','FlrPeakFlux','FlrIntFlux2','TypeII','TypeIV','optlocation','tchianti','emchianti','FlrIntFlux','sep']]\n",
    "dfTrain = feature_data_old\n",
    "\n",
    "# to add in the SEP events from 2004 to 2017 uncomment this section\n",
    "#feature_data_new = new_data[['FlrOnset','Flrmaxtime','FlrPeakFlux','FlrIntFlux2','TypeII','TypeIV','optlocation','tchianti','emchianti','FlrIntFlux','sep']]\n",
    "#dfTrain = pd.concat([feature_data_old,feature_data_new])\n",
    "\n",
    "#split dataset into training and test - not yet featurized - needs to be done after\n",
    "#statement regarding how many features to use\n",
    "labels = np.array(dfTrain.sep, dtype=int)\n",
    "X_train0, X_test0, y_train0, y_test0 = train_test_split(dfTrain,labels,\\\n",
    "                            test_size=0.2, random_state=1230, stratify=labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "\n",
    "# Logisitc Regression\n",
    "\n",
    "*****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize SEP object and create feature data\n",
    "\n",
    "The Logisitic Regression classifier to initialize an SEP object and generate the feature data for training and test sets.\n",
    "\n",
    "NOTE: remember to set sepLR.allmyfeatures.set_params(LocFeatures=None,RawFeat=None) correctly here and a few cells later after the gridsearchcv. Need to find away to update the object keywords with the gridsearchcv best_params without having to reintialze the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialize SEPClass instance with an estimator of choice - here Logisitic Regression\n",
    "#with estimator specific keywords \n",
    "keywords = {'random_state':1230, 'max_iter':150}\n",
    "\n",
    "#sep_weights = {0:12,1:39} #dictionary for experimental class weights\n",
    "sepLR = SEPClass(LogisticRegression(**keywords, class_weight = 'balanced', \\\n",
    "                C=0.9,verbose=0,solver='newton-cg'),threshold = 0.5)\n",
    "\n",
    "#Turn off features. Options: BalchFeat, RawFeat,LocFeatures\n",
    "#sepLR.allmyfeatures.set_params(LocFeatures=None,RawFeat=None)\n",
    "\n",
    "# convert each event into features and labels \n",
    "X_train = sepLR.build_train_features(X_train0)    \n",
    "y_train = y_train0\n",
    "\n",
    "X_test = sepLR.build_train_features(X_test0)    \n",
    "y_test = y_test0\n",
    "\n",
    "#standardization\n",
    "#X_train = preprocessing.scale(X_train)\n",
    "#X_test = preprocessing.scale(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypertune Logisitic Regression parameters with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up some scorers to work with GridSearchCV\n",
    "metric3 = make_scorer(matthews_corrcoef)\n",
    "\n",
    "def tss_score(y, y_pred):\n",
    "    tn, fp, fn, tp=confusion_matrix(y, y_pred).ravel()\n",
    "    tss = tp / (tp + fp + fn)\n",
    "    return tss\n",
    "\n",
    "def hss_score(y, y_pred):\n",
    "    tn, fp, fn, tp=confusion_matrix(y, y_pred).ravel()\n",
    "    E = ((tp + fn)*(tp + fp) + (fp + tn)*(fn + tn)) / X_train.shape[0]\n",
    "    HSS = (tp + tn - E)/(X_train.shape[0] - E)\n",
    "    return HSS\n",
    "\n",
    "metric5 = make_scorer(tss_score)\n",
    "metric6 = make_scorer(hss_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#hyperparameter ranges\n",
    "CV_dictLR = {'C':np.arange(0.0052,0.11,0.01),'class_weight':[None,'balanced'],\\\n",
    "           'solver':['liblinear','newton-cg','lbfgs']}\n",
    "\n",
    "##note passing SEPClass object sep directly to gridsearchcv\n",
    "cv_searchLR = GridSearchCV(estimator = sepLR.estimator, param_grid = CV_dictLR, scoring = metric5)   \n",
    "cv_searchLR.fit(X_train,y_train)\n",
    "cv_resultLR = cv_searchLR.best_params_\n",
    "print(cv_resultLR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the tuned hyperparameters to train the LR model\n",
    "\n",
    "Report card output with model parameters and skill score are saved into directory reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "\n",
    "#train the model\n",
    "keywordsLR.update(cv_resultLR)\n",
    "sepLR = SEPClass(LogisticRegression(**keywordsLR),threshold = 0.5)\n",
    "#sepLR.allmyfeatures.set_params(LocFeatures=None,RawFeat=None)\n",
    "sepLR.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numfeat = len(sepLR.allmyfeatures.get_feature_names())\n",
    "fname = 'sepLR_nfeat%i' % (numfeat)\n",
    "with open('reports/'+fname+'.txt', 'w') as f:\n",
    "    f.write(cap.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logisitic Regression Test set\n",
    "\n",
    "Now use the SEPClass predict method to make predictions on the test set and print out a report card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sep.predict(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration of model performance\n",
    "\n",
    "Show the top features for both the positive and negative class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sepLR.show_topX(num=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Miclassified events sorted by flare latitude and longitudinal position and flare SXR peak flux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create pandas dataframe with training features and y labels plus predictions\n",
    "feat_names = [n.split('__')[1] for n in sepLR.allmyfeatures.get_feature_names() ]\n",
    "dfFeatTrain = pd.DataFrame(data = X_train, columns = feat_names )\n",
    "dfFeatTrain['y'] = sepLR.y_train\n",
    "dfFeatTrain['y_pred'] = sepLR.y_train_pred\n",
    "\n",
    "#determine false alarms and missed events\n",
    "FA = dfFeatTrain.query('y == 0 and y_pred == 1')\n",
    "Miss = dfFeatTrain.query('y == 1 and y_pred == 0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#plot of flare Lat and Long vs SXR peak flux\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize = [15,5])\n",
    "fig.suptitle('SEP event Misses and False Alarms')\n",
    "ax1.scatter(FA.EW, FA.NS, marker = 'o',s = FA.FlrPeakFlux/dfFeatTrain.FlrPeakFlux.max()*1000,c='b', label = 'FA')\n",
    "ax1.scatter(Miss.EW, Miss.NS,s = Miss.FlrPeakFlux/dfFeatTrain.FlrPeakFlux.max()*1000, c='r', label = 'Miss')\n",
    "ax1.set_title('Flare SXR Peak')\n",
    "ax1.legend()\n",
    "\n",
    "#plot of flare Lat and Long vs SXR Integrated flux\n",
    "ax2.scatter(FA.EW, FA.NS, marker = 'o',s = FA.FlrIntFlux/dfFeatTrain.FlrIntFlux.max()*100,c='b'*1000, label = 'FA')\n",
    "ax2.scatter(Miss.EW, Miss.NS,s = FA.FlrIntFlux/dfFeatTrain.FlrIntFlux.max()*1000, c='r', label = 'Miss')\n",
    "ax2.set_title('Flare SXR Int Flux')\n",
    "ax2.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "# Adaboost with Decision Trees\n",
    "\n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialize SEPClass instance with an estimator of choice - here Decision Trees\n",
    "\n",
    "#Set up the decision tree classifier\n",
    "dtc_keywords = {'max_depth':1, 'max_features':None,'random_state':1230,'min_samples_split':2,'min_samples_leaf':1}\n",
    "dtc = trees(**dtc_keywords)\n",
    "\n",
    "#Set up the SEP class with the AdaBoost Classifier\n",
    "clf_keywords = {'random_state':1230}\n",
    "sepDT = SEPClass(AdaBoostClassifier(dtc,**clf_keywords))\n",
    "\n",
    "#Turn off features. Options: BalchFeat, RawFeat,LocFeatures\n",
    "#sepDT.allmyfeatures.set_params(RawFeat=None)\n",
    "\n",
    "# convert each event into features and labels \n",
    "X_train = sepDT.build_train_features(X_train0)    \n",
    "y_train = y_train0\n",
    "\n",
    "X_test = sepDT.build_train_features(X_test0)    \n",
    "y_test = y_test0\n",
    "\n",
    "#standardization\n",
    "#X_train = preprocessing.scale(X_train)\n",
    "#X_test = preprocessing.scale(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypertune Decision Tree parameters with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Tune the tree\n",
    "dtc_params = {'criterion':['gini','entropy']}\n",
    "dtc_search = GridSearchCV(dtc,dtc_params,scoring = metric5)\n",
    "dtc_search.fit(X_train,y_train)\n",
    "dtc_keywords.update(dtc_search.best_params_)\n",
    "print(dtc_search.best_params_)\n",
    "\n",
    "# Tune Adaboost\n",
    "dtc = trees(**dtc_keywords)\n",
    "clf = AdaBoostClassifier(dtc,**clf_keywords)\n",
    "clf_params = {'n_estimators':range(50,92,2),'learning_rate':[0.01,0.1,1.0]}\n",
    "clf_search = GridSearchCV(clf,clf_params, scoring = metric5)\n",
    "clf_search.fit(X_train,y_train)\n",
    "clf_keywords.update(clf_search.best_params_)\n",
    "print(clf_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the tuned hyperparameters to train the Adaboost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "\n",
    "#train the model\n",
    "dtc = trees(**dtc_keywords)\n",
    "sepDT = SEPClass(AdaBoostClassifier(dtc,**clf_keywords))\n",
    "#sepLR.allmyfeatures.set_params(RawFeat=None)\n",
    "sepDT.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numfeat = len(sepDT.allmyfeatures.get_feature_names())\n",
    "fname = 'sepDT_nfeat%i' % (numfeat)\n",
    "with open('reports/'+fname+'.txt', 'w') as f:\n",
    "    f.write(cap.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost Test set\n",
    "\n",
    "Now use the SEPClass predict method to make predictions on the test set and print out a report card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sepDT.predict(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration of Adaboost model performance\n",
    "\n",
    "Show the top features for both the positive and negative class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sepDT.show_topX(num=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Miclassified events sorted by flare latitude and longitudinal position and flare SXR peak flux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create pandas dataframe with training features and y labels plus predictions\n",
    "feat_names = [n.split('__')[1] for n in sepDT.allmyfeatures.get_feature_names() ]\n",
    "dfFeatTrain = pd.DataFrame(data = X_train, columns = feat_names )\n",
    "dfFeatTrain['y'] = sepDT.y_train\n",
    "dfFeatTrain['y_pred'] = sepDT.y_train_pred\n",
    "\n",
    "#determine false alarms and missed events\n",
    "FA = dfFeatTrain.query('y == 0 and y_pred == 1')\n",
    "Miss = dfFeatTrain.query('y == 1 and y_pred == 0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#plot of flare Lat and Long vs SXR peak flux\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize = [15,5])\n",
    "fig.suptitle('SEP event Misses and False Alarms')\n",
    "ax1.scatter(FA.EW, FA.NS, marker = 'o',s = FA.FlrPeakFlux/dfFeatTrain.FlrPeakFlux.max()*1000,c='b', label = 'FA')\n",
    "ax1.scatter(Miss.EW, Miss.NS,s = Miss.FlrPeakFlux/dfFeatTrain.FlrPeakFlux.max()*1000, c='r', label = 'Miss')\n",
    "ax1.set_title('Flare SXR Peak')\n",
    "ax1.legend()\n",
    "\n",
    "#plot of flare Lat and Long vs SXR Integrated flux\n",
    "ax2.scatter(FA.EW, FA.NS, marker = 'o',s = FA.FlrIntFlux/dfFeatTrain.FlrIntFlux.max()*100,c='b'*1000, label = 'FA')\n",
    "ax2.scatter(Miss.EW, Miss.NS,s = FA.FlrIntFlux/dfFeatTrain.FlrIntFlux.max()*1000, c='r', label = 'Miss')\n",
    "ax2.set_title('Flare SXR Int Flux')\n",
    "ax2.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "****\n",
    "# Balch SEP forecast\n",
    "\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SEP forecasts with machine learning techniques are compared to forecasts made using the technique outlined in the Balch 2008 paper. A statitical look up table is created for sep events and control events between 1986 and 2004. The sep event probability lookup table is 3 Dimensional with feature binning consisting of: \n",
    "\n",
    "5 categories of integrated flux: <0.026, 0.026-0.085, 0.085-0.275, 0.275-0.895, >0.895  \n",
    "5 categories of x-ray class: <= M2, M3-M8, M9-X1,X2-X6, >= X7  \n",
    "4 categories of radio type: None, type II, type IV, Both (if unknown, ignore radio event counts)  \n",
    "\n",
    "In instances where there are not enough sample events is not enough, the dimensionality of the lookup table is collased to give the required bin statistics.\n",
    "\n",
    "Validation metrics given in Balch 08 were mostly for the training data - where the model was validated using the same events that were used to develope the lookup table. A categorical HSS score of 0.47 was found. An effort to recover a test score was carried out by randomly splitting the data set into 2 groups, training on one half and testing on the other. 30 trials were conducted with a mean HSS of 0.46 +/- XXX. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sunpy.instr\n",
    "import sunpy\n",
    "import sunpy.instr\n",
    "import astropy.units as u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def intflux_ind(INTFLX):\n",
    "    \"\"\"Figure out the integrated flux 'index' \"\"\"\n",
    "    \n",
    "    if (INTFLX < 0.026): xi = 0;\n",
    "    if np.logical_and((0.026  <= INTFLX), (INTFLX < 0.085)):  xi = 1;\n",
    "    if np.logical_and((0.085  <= INTFLX), (INTFLX < 0.275)):  xi = 2;\n",
    "    if np.logical_and((0.275  <= INTFLX), (INTFLX < 0.895)):  xi = 3;\n",
    "    if (0.895  <= INTFLX): xi = 4;\n",
    "        \n",
    "    return xi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pkflux_ind(PKFLUX):\n",
    "\n",
    "    \"\"\"figure out the x-ray class index\"\"\"\n",
    "\n",
    "    #Flare peak flux to X-ray class\n",
    "    xrcls = sunpy.instr.goes.flux_to_flareclass(PKFLUX * u.watt/u.m**2)\n",
    "\n",
    "    #Get the class\n",
    "    cxray = xrcls[0]\n",
    "    \n",
    "    #numerical\n",
    "    nxray = int(xrcls[1])\n",
    "\n",
    "    #Calculate index\n",
    "    xci = 0;    # default\n",
    "    if (cxray == 'M'):\n",
    "        if (nxray <= 2): xci = 0\n",
    "        if np.logical_and((nxray > 2), (nxray <= 8)): xci = 1\n",
    "        if (nxray > 8): xci = 2\n",
    "            \n",
    "    if (cxray == 'X'):\n",
    "        if (nxray <=1): xci = 2\n",
    "        if np.logical_and((nxray > 1), (nxray <= 6)): xci = 3\n",
    "        if (nxray > 6): xci = 4\n",
    "            \n",
    "    return xci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def radio_burst_ind(typeII, typeIV, radiopatrol):\n",
    "    \n",
    "    \"\"\"figure out the radio burst index\"\"\"\n",
    "    \n",
    "    #print(\"# no radio \", len(np.where(radiopatrol == 0)))\n",
    "    #print(radiopatrol)\n",
    "    \n",
    "    if (int(radiopatrol) == 0): \n",
    "        #print(\"no radio patrol\")\n",
    "        ri = -1 \n",
    "    else:\n",
    "        if np.logical_and((typeII == 0),(typeIV == 0)): ri =  0 # none\n",
    "        if np.logical_and((typeII == 1),(typeIV == 0)): ri =  1 # type II only\n",
    "        if np.logical_and((typeII == 0),(typeIV == 1)): ri =  2 # type IV only\n",
    "        if np.logical_and((typeII == 1),(typeIV == 1)): ri =  3 # both\n",
    "\n",
    "    return ri\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sep_prob(data):\n",
    "\n",
    "    #probability of an sep event\n",
    "    sep_probability = np.zeros(len(data))\n",
    "    \n",
    "    #loop through all events (sep + ctrl) dataframe\n",
    "    for i in range(len(data)):    \n",
    "    \n",
    "        #determine the bin index for flare SXRintegrated flux\n",
    "        xi = intflux_ind(data.FlrIntFlux2.iloc[i])\n",
    "        \n",
    "        #determine the bin index for the flare peak flux\n",
    "        xci = pkflux_ind(data.FlrPeakFlux.iloc[i])\n",
    "        \n",
    "        #determine the bin for index for radio burts\n",
    "        ri = radio_burst_ind(data.TypeII.iloc[i], data.TypeIV.iloc[i], data.radiopatrol.iloc[i])\n",
    "        \n",
    "        #sep events\n",
    "        events = np.array([  \n",
    "        #INTFLX < 0.026...\n",
    "            [ [0, 0, 1, 0],    # X <= M2\n",
    "              [1, 0, 0, 0],    # X from M3-M8\t  \n",
    "              [0, 0, 0, 0],    # X from M9-X1\t  \n",
    "              [0, 0, 0, 0],    # X from X2-X6\t  \n",
    "              [0, 0, 0, 0]     # X >= X7\n",
    "            ], \n",
    "        #INTFLX from 0.026 to 0.085\n",
    "            [ [0, 0, 0, 3],    # X <= M2\n",
    "              [2, 0, 0, 3],    # X from M3-M8\t  \n",
    "              [1, 1, 0, 0],    # X from M9-X1\t  \n",
    "              [0, 0, 0, 0],    # X from X2-X6\t  \n",
    "              [0, 0, 0, 0]     # X >= X7\n",
    "            ],\n",
    "        #INTFLX from 0.085 to 0.275\n",
    "            [ [0, 1, 0, 0],    # X <= M2\n",
    "              [2, 2, 7, 6],    # X from M3-M8\t  \n",
    "              [1, 0, 0, 2],    # X from M9-X1\t  \n",
    "              [1, 0, 0, 3],    # X from X2-X6\t  \n",
    "              [0, 0, 0, 0]     # X >= X7\n",
    "            ],\n",
    "        #INTFLX from 0.275 to 0.895\n",
    "            [ [0, 1, 0, 0],    # X <= M2\n",
    "              [0, 1, 0, 3],    # X from M3-M8\t  \n",
    "              [2, 1, 2, 6],    # X from M9-X1\t  \n",
    "              [1, 0, 3,14],    # X from X2-X6\t  \n",
    "              [0, 1, 0, 5]     # X >= X7\n",
    "            ],\n",
    "        #INTFLX > 0.895\n",
    "            [ [0, 0, 0, 0],    # X <= M2\n",
    "              [0, 0, 0, 0],    # X from M3-M8\t  \n",
    "              [0, 0, 0, 0],    # X from M9-X1\t  \n",
    "              [1, 0, 3, 2],    # X from X2-X6\t  \n",
    "              [0, 0, 2, 3]     # X >= X7\n",
    "            ]\n",
    "            ])\n",
    "\n",
    "\n",
    "        samples = np.array([ \n",
    "        # INTFLX < 0.026...\n",
    "            [ [405, 26, 8, 8], # X <= M2\n",
    "              [56,  16, 2, 2], # X from M3-M8\t  \n",
    "              [0, 0, 0, 0],    # X from M9-X1\t  \n",
    "              [0, 0, 0, 0],    # X from X2-X6\t  \n",
    "              [0, 0, 0, 0]     # X >= X7\n",
    "            ],\n",
    "        # INTFLX from 0.026 to 0.085\n",
    "            [ [257, 15, 12, 8], # X <= M2\n",
    "              [133, 24, 9, 16], # X from M3-M8\t  \n",
    "              [38,  11, 4, 7],     # X from M9-X1\t  \n",
    "              [12,   1, 0, 2],     # X from X2-X6\t  \n",
    "              [0,    0, 0, 0]      # X >= X7\n",
    "            ],\n",
    "        # INTFLX from 0.085 to 0.275\n",
    "            [ [34,  2,  4,  2], # X <= M2\n",
    "              [48, 11, 12, 16], # X from M3-M8\t  \n",
    "              [36, 10,  4, 12],    # X from M9-X1\t  \n",
    "              [18,  9,  4, 10],    # X from X2-X6\t  \n",
    "              [0,   0,  0,  0]     # X >= X7\n",
    "            ],\n",
    "        # INTFLX from 0.275 to 0.895\n",
    "            [ [1,  0, 0,  0],    # X <= M2\n",
    "              [1,  2, 2,  4],    # X from M3-M8\t  \n",
    "              [11, 1, 4,  8],    # X from M9-X1\t  \n",
    "              [14, 5, 4, 23],    # X from X2-X6\t  \n",
    "              [0,  1, 0,  7]     # X >= X7\n",
    "            ],\n",
    "        # INTFLX > 0.895\n",
    "            [ [0, 0, 0,  0],    # X <= M2\n",
    "              [1, 0, 0,  0],    # X from M3-M8\t  \n",
    "              [1, 0, 0,  0],    # X from M9-X1\t  \n",
    "              [2, 0, 3,  5],    # X from X2-X6\t  \n",
    "              [0, 0, 2, 10]     # X >= X7\n",
    "            ]\n",
    "        ])\n",
    "\n",
    "        #print(events.shape)\n",
    "        #print(samples.shape)\n",
    "        \n",
    "        \n",
    "        #If radiotype is unknown or sample size is less than ten\n",
    "        #it is required to collapse the radio counts\n",
    "        if np.logical_or((ri == -1),(samples[xi, xci, ri] < 10)):\n",
    "            #print(\"samples less than 10\")\n",
    "            #if (ri == -1):\n",
    "                #print(\"unknown radio\")\n",
    "            \n",
    "            samples2 = 0\n",
    "            events2 = 0\n",
    "            for i in range(4):\n",
    "                samples2 += samples[xi, xci, i]\n",
    "                events2 += events[xi, xci, i]\n",
    "\n",
    "            # It may be necessary to collapse the xray class as well           \n",
    "            if (samples2 < 10):\n",
    "                #print(\"samples2 less than 10\")\n",
    "                samples3 = 0\n",
    "                events3 = 0\n",
    "                for i in range(5):\n",
    "                    for j in range(4):\n",
    "                        samples3 += samples[xi, i, j]\n",
    "                        events3 += events[xi, i, j]\n",
    "                        \n",
    "                #return the probability estimate\n",
    "                if samples3 > 0:\n",
    "                    sep_probability[i] =  1.0*events3/samples3;\n",
    "                else:\n",
    "                    print(\"nan\")\n",
    "                    sep_probability[i] = np.nan\n",
    "\n",
    "            sep_probability[i] = 1.0*events2/samples2;\n",
    "\n",
    "        sep_probability[i] = 1.0*events[xi, xci, ri]/samples[xi, xci, ri]\n",
    "        \n",
    "    return sep_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#metrics\n",
    "\n",
    "#TSS\n",
    "def tss_score(y, y_pred):\n",
    "    tn, fp, fn, tp=confusion_matrix(y, y_pred).ravel()\n",
    "    tss = tp / (tp + fp + fn)\n",
    "    return tss\n",
    "\n",
    "\n",
    "#HSS\n",
    "def hss_score(y, y_pred):\n",
    "    tn, fp, fn, tp=confusion_matrix(y, y_pred).ravel()\n",
    "    E = ((tp + fn)*(tp + fp) + (fp + tn)*(fn + tn)) / len(y)\n",
    "    HSS = (tp + tn - E)/(len(y) - E)\n",
    "    return HSS\n",
    "        \n",
    "def pod_far_pc(y, y_pred):\n",
    "    tn, fp, fn, tp=confusion_matrix(y, y_pred).ravel()\n",
    "    \n",
    "    pod = tp / (tp + fn)\n",
    "    far = fp / (tp + fp)\n",
    "    pc = (tp + tn)/ len(y)\n",
    "    \n",
    "    return pod, far, pc  \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read in the original Balch SEP event list with added CME speed and GOES T and EM\n",
    "orig_data = pd.read_excel(\"ctrlevents.v8p.xls\")\n",
    "\n",
    "#adding y label column indicating positive and negative SEP events - SEP events have Association = ProtonFlare\n",
    "orig_data['sep'] = orig_data.Association.str.contains('^Proton').astype(int)\n",
    "\n",
    "#rename radiopatrol\n",
    "orig_data.rename(columns={\"Rpatrol\":'radiopatrol'}, inplace=True)\n",
    "\n",
    "#radiopatrol to int\n",
    "orig_data.radiopatrol = (orig_data.radiopatrol.str.lower() == \"yes\").astype(int)\n",
    "\n",
    "#convert the type II and type IV association to binary \n",
    "orig_data.TypeII = (orig_data.TypeII.str.lower() == \"yes\").astype(int)\n",
    "orig_data.TypeIV = (orig_data.TypeIV.str.lower() == \"yes\").astype(int)\n",
    "\n",
    "# Remove rows where optlocation = nan\n",
    "#orig_data = orig_data[orig_data.optlocation.astype('str') != 'nan']\n",
    "\n",
    "#shuffle the events so they are not organized \n",
    "#orig_data = shuffle(orig_data)\n",
    "\n",
    "#save the shuffled dataframe -- commented out to prevent resaving\n",
    "#orig_data.to_csv(\"AllEvtsShuffled_1986_2004.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hazelbain/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:132: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/Users/hazelbain/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:130: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Pos: 30, True Neg: 3611, False Pos: 45,. False Neg: 97\n",
      "\n",
      "TSS: 0.1744, HSS:0.2791\n",
      "POD: 0.2362, FAR:0.6000, PC:0.9625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hazelbain/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:34: RuntimeWarning: invalid value encountered in greater\n"
     ]
    }
   ],
   "source": [
    "#read in the shuffled dataframe of original sep+ctrl events from 1986 - 2004\n",
    "#orig_data = pd.read_csv(\"AllEvtsShuffled_1986_2004_cme.csv\")\n",
    "\n",
    "#pull out feature columns\n",
    "data_B08 = orig_data[['FlrOnset','Flrmaxtime','FlrPeakFlux','FlrIntFlux2','TypeII','TypeIV','optlocation','FlrIntFlux','radiopatrol','sep']]\n",
    "#data_B08 = orig_data[['FlrOnset','Flrmaxtime','FlrPeakFlux','FlrIntFlux2','TypeII','TypeIV','optlocation','tchianti','emchianti','FlrIntFlux','radiopatrol','sep']]\n",
    "#data_B08 = data_B08.query('radiopatrol == 1')\n",
    "\n",
    "#Use Sep object with LogReg just to featurize the data with the location\n",
    "keywords = {'random_state':1230, 'max_iter':150}\n",
    "\n",
    "sepLR = SEPClass(LogisticRegression(**keywords),threshold = 0.5)\n",
    "\n",
    "#Turn off features. Options: BalchFeat, RawFeat,LocFeatures\n",
    "sepLR.allmyfeatures.set_params(LocFeatures = None, RawFeat=None)\n",
    "\n",
    "# convert each event into features and labels \n",
    "X_train_B08 = sepLR.build_train_features(data_B08)    \n",
    "y_train_B08 = np.array(data_B08.sep, dtype=int)\n",
    "\n",
    "feat_names = [n.split('__')[1] for n in sepLR.allmyfeatures.get_feature_names() ]\n",
    "dfXtrain08 = pd.DataFrame(data = X_train_B08, columns = feat_names )\n",
    "dfXtrain08['sep'] = y_train_B08\n",
    "dfXtrain08['radiopatrol'] = orig_data.radiopatrol\n",
    "\n",
    "#how manhy events occur behind the limb\n",
    "#dfXtrain08.query('EW > 90.0 or EW < -90.0')\n",
    "\n",
    "#caluclate sep event probabilities from balch 2008 table\n",
    "sp = sep_prob(dfXtrain08)\n",
    "\n",
    "#compare with actual labels\n",
    "threshold = 0.3\n",
    "sep_class = (sp > threshold).astype(int)\n",
    "\n",
    "#add to data frame\n",
    "dfXtrain08['sep_balch'] = sep_class.tolist()\n",
    "\n",
    "#confusion matrix\n",
    "tn, fp, fn, tp=confusion_matrix(dfXtrain08.sep, dfXtrain08.sep_balch).ravel()\n",
    "print(\"True Pos: %i, True Neg: %i, False Pos: %i,. False Neg: %i\\n\" % (tp,tn,fp,fn))\n",
    "\n",
    "tss = tss_score(dfXtrain08.sep, dfXtrain08.sep_balch)\n",
    "hss = hss_score(dfXtrain08.sep, dfXtrain08.sep_balch)\n",
    "pod, far, pc = pod_far_pc(dfXtrain08.sep, dfXtrain08.sep_balch)\n",
    "\n",
    "\n",
    "\n",
    "print(\"TSS: %.4f, HSS:%.4f\" % (tss, hss))\n",
    "print(\"POD: %.4f, FAR:%.4f, PC:%.4f\" % (pod, far, pc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tp = 60\n",
    "#fn = 67\n",
    "#tn = 3611\n",
    "#fp = 45\n",
    "\n",
    "#E = ((tp + fn)*(tp + fp) + (fp + tn)*(fn + tn)) / len(orig_data)\n",
    "#HSS = (tp + tn - E)/(len(orig_data) - E)\n",
    "\n",
    "#pod = tp / (tp + fn)\n",
    "#far = fp / (tp + fp)\n",
    "#pc = (tp + tn)/ len(orig_data)\n",
    "    \n",
    "#print(\"HSS:%.4f\" % (HSS))\n",
    "#print(\"POD: %.4f, FAR:%.4f, PC:%.4f\" % (pod, far, pc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plot\n",
    "plt.scatter(np.arange(len(data)),sp)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
