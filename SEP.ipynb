{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEP Forecasting \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current physics based models of SEPs are unable to execute sufficiently fast in order to\n",
    "provide actionable information towards forecasting such disturbances, which can impact\n",
    "Earth within tens of minutes of the onset of an eruptive event. This is compounded by\n",
    "the intrinsic latency of certain key observations, which are used to define the initial\n",
    "conditions of these models. Instead, there is a reliance on statistical models to provide\n",
    "forecast probabilities of Earth-bound SEPs using real-time data. Since the largest, most\n",
    "impactful events occur infrequently, some regions of the feature space are sparse and\n",
    "simple discrete binning procedures have limitations. The goal of this project is to\n",
    "improve upon the empirical SEP proton prediction forecast model (PROTONS) currently\n",
    "in operational use at SWPC, through the application of modern machine learning\n",
    "techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "from datetime import datetime,timedelta\n",
    "\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, GridSearchCV\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, roc_curve\n",
    "from sklearn.metrics import brier_score_loss as bsl\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "import scikitplot as skplt\n",
    "\n",
    "from scipy.sparse import csr_matrix, find\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading in the SEP and control data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FlrOnset',\n",
       " 'Flrmaxtime',\n",
       " 'Flrendtime',\n",
       " 'FlrPeakFlux',\n",
       " 'Flareq',\n",
       " 'xrsclass',\n",
       " 'optclass',\n",
       " 'optlocation',\n",
       " 'region',\n",
       " 'radiopatrol',\n",
       " 'TypeII',\n",
       " 'TypeIIDur',\n",
       " 'TypeIV',\n",
       " 'TypeIVDur',\n",
       " 'cmepatrol',\n",
       " 'cmeonset',\n",
       " 'cmespeed',\n",
       " 'satid',\n",
       " 'FlrHpTime',\n",
       " 'FlrIntFlux',\n",
       " 'FlrIntFlux2',\n",
       " 'IntFluxQ',\n",
       " 'tmewe',\n",
       " 'emmewe',\n",
       " 'tchianti',\n",
       " 'emchianti',\n",
       " 'Association',\n",
       " 'S1',\n",
       " 'S2',\n",
       " 'S3',\n",
       " 'S4',\n",
       " 'S5',\n",
       " 'GT100']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read in the Balch SEP event list\n",
    "#sepdata = pd.read_excel(\"SPEall.v7p.xls\")\n",
    "\n",
    "#read in the Balch SEP event list\n",
    "#data = pd.read_excel(\"ctrlevents.v8p.xls\")\n",
    "\n",
    "#read in the student event list\n",
    "data = pd.read_excel(\"ControlEvents_student.xls\")\n",
    "list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding y label column indicating positive and negative SEP events - SEP events have Association = ProtonFlare\n",
    "#data['sep'] = (data.Association == \"ProtonFlare\").astype(int)\n",
    "data['sep'] = data.Association.str.contains('^Proton').astype(int)\n",
    "\n",
    "# Remove rows where optlocation = nan\n",
    "data = data[data.optlocation.astype('str') != 'nan']\n",
    "\n",
    "#shuffle the events so they are not organized \n",
    "data = shuffle(data)\n",
    "\n",
    "# Remove rows where optlocation = nan\n",
    "data = data[data.optlocation.astype('str') != 'nan']\n",
    "\n",
    "#save the shuffled dataframe -- commented out to prevent resaving\n",
    "data.to_csv(\"AllEvtsShuffled_student.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make Scatterplot matrix showing relationship between parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pd.plotting.scatter_matrix(data[['Maxflux','FlrPeakFlux','TII_Duration','TIV_duration',\\\n",
    "#                            'FlrIntFlux','FlrIntFlux2','P10MAX','P30MAX','P60MAX',\\\n",
    "#                            'P100MAX','P10FLUENCE','P30FLUENCE','P60FLUENCE','P100FLUENCE']], \\\n",
    "#                            figsize=(18,18), diagonal='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logisitic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class to exract features from dataframe that can be used directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BalchPaperFeatures(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        \"\"\"Class to create original 4 features from Balch 2008\"\"\"\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def fit(self, examples):\n",
    "        # return self and nothing else \n",
    "        return self\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        \"\"\"Array mapping from feature integer indices to feature name\"\"\"\n",
    "        \n",
    "        return ['FlrPeakFlux','FlrIntFlux','TypeII','TypeIV']\n",
    "    \n",
    "    def transform(self, examples):\n",
    "                \n",
    "        #Choose the orginal 4 Balch 2008 features (5 if you include both type II and type Iv as separate features)\n",
    "        #X = examples[['FlrPeakFlux','FlrIntFlux','Type24']]\n",
    "        X = examples[['FlrPeakFlux','FlrIntFlux','TypeII','TypeIV']]\n",
    "\n",
    "        return(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RawFeatures(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        \"\"\"Ogher features from the Event List that don't need to be manipulated before being included in model\"\"\"\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def fit(self, examples):\n",
    "        # return self and nothing else \n",
    "        return self\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        \"\"\"Array mapping from feature integer indices to feature name\"\"\"\n",
    "        \n",
    "        return ['FlrPeakFlux','FlrIntFlux','TypeII','TypeIV']\n",
    "    \n",
    "    def transform(self, examples):\n",
    "                \n",
    "        #Choose the orginal 4 Balch 2008 features (5 if you include both type II and type Iv as separate features)\n",
    "        X = examples[['FlrIntFlux2','TypeIIDur','TypeIVDur','tchianti','emchianti']]\n",
    "\n",
    "        return(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class FlareTime2Peak(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    \"\"\"Class to create feature with the time between flare onset to flare max time\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def fit(self, examples):\n",
    "        # return self and nothing else \n",
    "        return self\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        \"\"\"Array mapping from feature integer indices to feature name\"\"\"\n",
    "        \n",
    "        return [\"FlTim2Pk\"]\n",
    "    \n",
    "    def transform(self, examples):\n",
    "        \n",
    "        # Initiaize matrix \n",
    "        X = np.zeros((len(examples),1))           \n",
    "\n",
    "        #time between flare max and flare onset\n",
    "        X[:,0] = np.asarray([(mx - on).seconds for mx, on in zip(examples.Flrmaxtime, examples.FlrOnset)])\n",
    "        \n",
    "        return(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LocationFeatures(BaseEstimator, TransformerMixin):\n",
    "   \n",
    "    \"\"\"Class to create feature with the time between flare onset to flare max time\"\"\"\n",
    "   \n",
    "    def __init__(self):\n",
    "       \n",
    "        return None\n",
    "   \n",
    "    def fit(self, examples):\n",
    "        # return self and nothing else \n",
    "        return self\n",
    "   \n",
    "    def get_feature_names(self):\n",
    "        \"\"\"Array mapping from feature integer indices to feature name\"\"\"\n",
    "       \n",
    "        return ['Loc']\n",
    "   \n",
    "    def transform(self, examples):\n",
    "       \n",
    "        # Initiaize matrix \n",
    "        X = np.zeros((len(examples),2))  \n",
    "       \n",
    "        #time between flare max and flare onset\n",
    "        #X[:,0] = np.asarray([str(x)[1:3] for x in examples.optlocation])   #north - south\n",
    "        #X[:,1] = np.asarray([str(x)[4::] for x in examples.optlocation])   #north - south\n",
    "       \n",
    "        for i,loc in enumerate(examples.optlocation):\n",
    "            lat = str(loc)[1:3]\n",
    "            if str(loc)[0] == 's' or str(loc)[0] == 'S':\n",
    "                X[i,0] = -int(lat)\n",
    "            else:\n",
    "                X[i,0] = (lat)\n",
    "\n",
    "            #west-east\n",
    "            long = str(loc)[4::]\n",
    "            if str(loc)[3] == 'w' or str(loc)[3] == 'W':\n",
    "                X[i,1] = -int(long)\n",
    "            else:\n",
    "                X[i,1] = int(long)\n",
    "\n",
    "        return(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEPClass(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, estimator, folds = 3, threshold=0.5):        # <--- other keywords to be used by Feature Union go here\n",
    "        \n",
    "        \"\"\"Class to fit and train Logistic Regression algorithm for SEP forecasting\n",
    "        \n",
    "        Input keywords:\n",
    "        \n",
    "        folds:        Number of cross validation folds to use\n",
    "        threshold:    Decision Boundary threshold\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.estimator = estimator     #estimator to use for classification e.g. LogReg or SVM\n",
    "        self.folds = folds             #cross validation folds for estimator\n",
    "        self.threshold = threshold     #decision boundary threshold\n",
    "    \n",
    "        #Set up the Feature union to combine Feature creating classes\n",
    "        self.allmyfeatures = FeatureUnion([\n",
    "            (\"BalchFeat\", BalchPaperFeatures()),\n",
    "            (\"RawFeat\", RawFeatures()),\n",
    "            (\"LocFeatures\", LocationFeatures()),\n",
    "            (\"FlareTime2Peak\", FlareTime2Peak())   \n",
    "        ])\n",
    "    \n",
    "    def build_train_features(self, examples):\n",
    "        \"\"\"\n",
    "        Method to take in training text features and do further feature engineering \n",
    "        Most of the work in this homework will go here, or in similar functions  \n",
    "        :param examples: currently just a list of forum posts  \n",
    "        \"\"\"  \n",
    "        \n",
    "        ##convert columns of time from string to datetime -- MOVE TO SEPARATE CLEANING FUNCTION\n",
    "        examples.FlrOnset = pd.Series(datetime.strptime(t, \"%Y-%m-%dT%H:%M:%S.%f\") for t in examples.FlrOnset)\n",
    "        examples.Flrmaxtime = pd.Series(datetime.strptime(t, \"%Y-%m-%dT%H:%M:%S.%f\") for t in examples.Flrmaxtime)\n",
    "        examples.Flrendtime = pd.Series(datetime.strptime(t, \"%Y-%m-%dT%H:%M:%S.%f\") for t in examples.Flrendtime)\n",
    "        #examples.FlrHpTime = pd.Series(datetime.strptime(t, \"%Y-%m-%dT%H:%M:%S.%f\") for t in examples.FlrHpTime)\n",
    "        \n",
    "        ##convert Type II and Type IV \"yes/no\" to binary\n",
    "        #print(examples.TypeII)\n",
    "        examples.TypeII = (examples.TypeII.str.lower() == \"yes\").astype(int)\n",
    "        examples.TypeIV = (examples.TypeIV.str.lower() == \"yes\").astype(int)\n",
    "        examples['Type24'] = np.logical_or(examples.TypeII, examples.TypeIV)\n",
    "        \n",
    "        return self.allmyfeatures.fit_transform(examples)\n",
    "        \n",
    "\n",
    "    def get_test_features(self, examples):\n",
    "        \"\"\"\n",
    "        Method to take in test text features and transform the same way as train features \n",
    "        :param examples: currently just a list of forum posts  \n",
    "        \"\"\"\n",
    "    \n",
    "        return self.allmyfeatures.transform(examples)\n",
    "\n",
    "    def show_topX(self, num=3):\n",
    "        \"\"\"\n",
    "        prints the top num features for the positive class and the \n",
    "        top 10 features for the negative class. \n",
    "        \"\"\"\n",
    "        feature_names = np.asarray([x.split(\"__\")[1] for x in self.allmyfeatures.get_feature_names()])\n",
    "        topX = np.argsort(self.estimator.coef_[0])[-num:]\n",
    "        bottomX = np.argsort(self.estimator.coef_[0])[:num]\n",
    "        \n",
    "        print(\"\\nTop 3 features for Pos and Neg\\n-------------------------\")\n",
    "        for fn in np.arange(1,num):\n",
    "            print(\"Pos %i: %s %f\" % (fn, feature_names[topX[-fn]], self.estimator.coef_[0,topX[-fn]]))\n",
    "        for fn in np.arange(0,num-1):\n",
    "            print(\"Neg %i: %s %f\" % (fn, feature_names[bottomX[fn]], self.estimator.coef_[0,bottomX[fn]]))\n",
    "      \n",
    "    def show_misclassified(self):     \n",
    "\n",
    "        \"\"\"\n",
    "        Method to show the misclassified examples i.e. False Positives and False negatives \n",
    "        \"\"\"\n",
    "        \n",
    "        #get all the feature names\n",
    "        words = feat.allmyfeatures.get_feature_names()\n",
    "        \n",
    "        # False positives\n",
    "        print(\"\\nSome misclassified examples:\")\n",
    "        falsepos = np.where((self.train_pred != self.y_train) & (self.train_pred == 1))[0]   #all false pos example rows\n",
    "        print(\"\\nPredicted SEP but labeled AllClear (False Pos) \\n------------------------- \")\n",
    "\n",
    "        for i in range(len(falsepos[0:10])):         #loop through falsepos examples\n",
    "            weights_falsepos = []\n",
    "            x = find(feat.X_train[falsepos[i]])      #find which features are used for this example\n",
    "            for ii in x[1]:                          #from sparse matrix get column indices corresponding to features\n",
    "                weights_falsepos.append((words[ii].split('__')[1], self.estimator.coef_[0,ii]))      #get the word and weight\n",
    "\n",
    "            print(\"label: %i, prediction %i, Neg Prob: %f, Pos Prob: %f, Ex No.: %i,  example: %s \" % \\\n",
    "                (self.y_train[falsepos[i]], self.train_pred[falsepos[i]] , self.train_pred_prob[falsepos[i]][0], \\\n",
    "                     self.train_pred_prob[falsepos[i]][1], falsepos[i], self.clean_examples[falsepos[i]]))\n",
    "            for j in weights_falsepos:\n",
    "                print(j)\n",
    "                \n",
    "        # False Negatives\n",
    "        falseneg = np.where((self.train_pred != self.y_train) & (self.train_pred == 0))[0]\n",
    "        print(\"\\nPredicted AllClear but labeled SEP (False Neg) \\n-------------------------\")\n",
    "\n",
    "        for i in range(len(falseneg[0:10])):\n",
    "            weights_falseneg = []\n",
    "            x = find(feat.X_train[falseneg[r]])\n",
    "            for ii in x[1]:\n",
    "                weights_falseneg.append((words[ii].split('__')[1], self.estimator.coef_[0,ii])) \n",
    "\n",
    "            print(\"label: %i, prediction %i, Neg Prob: %f, Pos Prob: %f, Ex No.: %i, \\nexample: %s \" % \\\n",
    "                (self.y_train[falseneg[i]], self.train_pred[falseneg[i]], self.train_pred_prob[falseneg[i]][0], \\\n",
    "                 self.train_pred_prob[falseneg[i]][1], falseneg[i], self.clean_examples[falseneg[i]]))\n",
    "            for j in weights_falseneg:\n",
    "                print(j)\n",
    "        \n",
    "        print(\"\\n\\n\")\n",
    "       \n",
    "    def show_report(self):\n",
    "        \n",
    "        \"\"\"Method to show a report card of the model fit \"\"\"\n",
    "        \n",
    "        # Add confusion Matrix\n",
    "        tn, fp, fn, tp=confusion_matrix(self.y_train, self.y_train_pred).ravel()\n",
    "        print(\"True Pos: %i, True Neg: %i, False Pos: %i,. False Neg: %i\\n\" % (tp,tn,fp,fn))\n",
    "\n",
    "        #calculate the mean square error\n",
    "        mserr = mse(self.y_train, self.y_train_pred)\n",
    "        print(\"MSE: %.4f\" % mserr)\n",
    "\n",
    "        #calculate the Brier score - is this the same as the MSE? And QR referenced in Balch paper?\n",
    "        bsloss = bsl(self.y_train, self.y_train_pred_prob[:,1])\n",
    "        print(\"BSL: %.4f\" % bsloss)\n",
    "\n",
    "        #Occurance rate = #SEPS / #events\n",
    "        occ_rate = self.y_train.sum()/len(self.y_train)\n",
    "\n",
    "        #Reference score of predicting all negative class\n",
    "        #QR_star = 0.0324 Balch\n",
    "        QR_star =  mse(self.y_train, np.zeros(len(self.y_train)))\n",
    "        print(\"RefQuadScore: %.4f\\n\" % QR_star)\n",
    "\n",
    "        #assume the QR is the same as the MSE - as stated later in Balch\n",
    "        QR = mserr\n",
    "        #QR = 0.0250 Balch\n",
    "\n",
    "        #skill score \n",
    "        SS = (QR_star - QR)/QR_star\n",
    "        print(\"SS: %.4f\" % SS)\n",
    "\n",
    "        FAR = fp/(tp + fp)\n",
    "        POD = tp/(tp + fn)\n",
    "\n",
    "        print(\"FAR: %.4f\" % FAR)\n",
    "        print(\"POD: %.4f\" % POD)\n",
    "\n",
    "        ##ROC metrics\n",
    "        fpr, tpr, thresh = roc_curve(self.y_train, self.y_train_pred, drop_intermediate=False)\n",
    "\n",
    "    def roc_curve(self):\n",
    "        \n",
    "        \"\"\"plot and ROC curve\"\"\"\n",
    "        \n",
    "        # Initial implementation of ROC plot applied to training set\n",
    "        skplt.metrics.plot_roc_curve(self.y_train, self.y_train_probas)\n",
    "        \n",
    "    def score(self, X, y):\n",
    "\n",
    "        \"\"\"find the accuracy score given the training data and labels\"\"\"\n",
    "        #print(\"...In Score...\")\n",
    "        #print(\"threshold:\", self.threshold)\n",
    "        \n",
    "        y_pred = (self.estimator.predict(X) > self.threshold).astype(int)\n",
    "        \n",
    "        #tn, fp, fn, tp=confusion_matrix(y, y_pred).ravel()\n",
    "        #return tp/(tp+fp+fn)\n",
    "        \n",
    "        return accuracy_score(y, y_pred)\n",
    "        \n",
    "    def fit(self, X, y):    #, random_state=1234):\n",
    "        \"\"\"\n",
    "        Method to read in training data from file, and \n",
    "        train Logistic Regression classifier. \n",
    "        \n",
    "        :param random_state: seed for random number generator \n",
    "        \"\"\"\n",
    "        \n",
    "        from sklearn.linear_model import LogisticRegression \n",
    "        \n",
    "        # load data \n",
    "        #self.dfTrain = pd.read_csv(\"AllEvtsShuffled.csv\")\n",
    "                      \n",
    "        # get training features and labels \n",
    "        #self.X_train = self.build_train_features(self.dfTrain)    #CHANGE\n",
    "        #self.y_train = np.array(self.dfTrain.sep, dtype=int)\n",
    "        \n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        \n",
    "        #print the shape of the features\n",
    "        print(\"Shape of the Features: Num examples x Num Features\")\n",
    "        print(self.X_train.shape)\n",
    "        #print(\"examples...:\", self.X_train[0:10])\n",
    "\n",
    "        #self.logreg.fit(self.X_train, self.y_train)\n",
    "        self.estimator.fit(self.X_train, self.y_train)\n",
    "        \n",
    "        # make predictions on training data \n",
    "        self.y_train_pred = self.estimator.predict(self.X_train)\n",
    "\n",
    "        #return the LogReg probabilities used to classify each example  \n",
    "        self.y_train_pred_prob = self.estimator.predict_proba(self.X_train)\n",
    "        \n",
    "        #print(\"\\nTraining Set\\n-------------------------\")\n",
    "        #self.score(self.train_pred, self.y_train)\n",
    "             \n",
    "        #cross validation\n",
    " \n",
    "        #print(\"\\nCross Validation Accuracy Scores (cross_val_score)\\n-------------------------\")\n",
    "        #scores = cross_val_score(self.estimator, self.X_train, self.y_train, cv=self.folds)\n",
    "        #print(scores)\n",
    "        #print(\"\\nMean Accuracy in Cross-Validation = %.3f \\n\" % scores.mean())\n",
    "        \n",
    "        #cross validation\n",
    "        #print(\"\\nCross Validation Accuracy Scores (cross_val_predict)\\n-------------------------\")\n",
    "        #self.y_pred = cross_val_predict(self.estimator, self.X_train, self.y_train, cv=self.folds)\n",
    "        #print(self.score(self.y_pred, self.y_train))\n",
    "        \n",
    "        #print(\"\\nCross Confusion Matrix\\n-------------------------\")\n",
    "        #self.conf_mat = confusion_matrix(self.y_train,self.y_pred)\n",
    "        #tn, fp, fn, tp = self.conf_mat.ravel()\n",
    "        #print(\"True Pos: %i, True Neg: %i, False Pos: %i,. False Neg: %i\" % (tp,tn,fp,fn))\n",
    "\n",
    "        \n",
    "    def predict(X):\n",
    "        \n",
    "        \"\"\"\n",
    "        Return predicted labels for exmaples X. \n",
    " \n",
    "        #### CURRENTLY THIS FUNCTION ISN'T USED - but could be called if we need predicted y vals \n",
    "        #### independently to the score function\n",
    " \n",
    "        \"\"\"\n",
    "        print(\"INside predict....\")\n",
    "        print(\"Thresh: \", self.threshold)\n",
    "        return (self.estimator.predict(X) > self.threshold).astype(int)\n",
    "        \n",
    "    def model_predict(self):\n",
    "        \"\"\"\n",
    "        Method to read in test data from file, make predictions\n",
    "        using trained model, and dump results to file \n",
    "        \n",
    "        #### CURRENTLY THIS FUNCTION ISN'T USED - leftover from FeatEngr homework but we might need to\n",
    "        #### to test on the holy grail test set\n",
    "        \"\"\"\n",
    "        \n",
    "        # read in test data \n",
    "        dfTest  = pd.read_csv(\"../data/spoilers/test.csv\")\n",
    "        \n",
    "        # featurize test data \n",
    "        self.X_test = self.get_test_features(list(dfTest[\"sentence\"]))\n",
    "        \n",
    "        # make predictions on test data \n",
    "        pred = self.estimator.predict(self.X_test)\n",
    "        \n",
    "        # dump predictions to file for submission to Kaggle  \n",
    "        #pd.DataFrame({\"spoiler\": np.array(pred, dtype=bool)}).to_csv(\"prediction.csv\", index=True, index_label=\"Id\")\n",
    "        \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the Features: Num examples x Num Features\n",
      "(3957, 12)\n"
     ]
    }
   ],
   "source": [
    "#Initialize SEPClass instance with an estimator of choice - here Logisitic Regression\n",
    "#with estimator specific keywords \n",
    "keywords = {'random_state':1230, 'max_iter':150}\n",
    "sep = SEPClass(LogisticRegression(**keywords) , threshold=0.5)\n",
    "\n",
    "# load data - for the gridsearcv the data needs to be loaded outside of the class\n",
    "dfTrain = pd.read_csv(\"AllEvtsShuffled_student.csv\")\n",
    "                      \n",
    "# get training features and labels \n",
    "X_train = sep.build_train_features(dfTrain)    #CHANGE\n",
    "y_train = np.array(dfTrain.sep, dtype=int)\n",
    "\n",
    "#Turn off features that aren't fully fleshed out or don't work\n",
    "#sep.allmyfeatures.set_params(RawFeat=None)\n",
    "\n",
    "#train the model\n",
    "sep.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Object parameters:\n",
      "----------------------------------------\n",
      "\n",
      "<bound method BaseEstimator.get_params of SEPClass(estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=150, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1230, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False),\n",
      "     folds=3, threshold=0.5)>\n",
      "\n",
      "Features:\n",
      "----------------------------------------\n",
      "\n",
      "['BalchFeat__FlrPeakFlux', 'BalchFeat__FlrIntFlux', 'BalchFeat__TypeII', 'BalchFeat__TypeIV', 'RawFeat__FlrPeakFlux', 'RawFeat__FlrIntFlux', 'RawFeat__TypeII', 'RawFeat__TypeIV', 'LocFeatures__Loc', 'FlareTime2Peak__FlTim2Pk']\n",
      "\n",
      "Report Card:\n",
      "----------------------------------------\n",
      "\n",
      "True Pos: 29, True Neg: 3806, False Pos: 13,. False Neg: 109\n",
      "\n",
      "MSE: 0.0308\n",
      "BSL: 0.0235\n",
      "RefQuadScore: 0.0349\n",
      "\n",
      "SS: 0.1159\n",
      "FAR: 0.3095\n",
      "POD: 0.2101\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#print report card\n",
    "\n",
    "\n",
    "print(\"\\nObject parameters:\\n----------------------------------------\\n\")\n",
    "print(sep.get_params)\n",
    "print(\"\\nFeatures:\\n----------------------------------------\\n\")\n",
    "print(sep.allmyfeatures.get_feature_names())\n",
    "print(\"\\nReport Card:\\n----------------------------------------\\n\")\n",
    "print(sep.show_report())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.964884020619\n",
      "{'estimator__max_iter': 100, 'threshold': 0.5}\n"
     ]
    }
   ],
   "source": [
    "#this Cell runs the GridSearchCV \n",
    "\n",
    "#this line sets up the SEP class with a logistic regression model with keyword arguements keywords\n",
    "keywords = {'random_state':1230}\n",
    "model_to_set = SEPClass(LogisticRegression(**keywords))\n",
    "#model_to_set.allmyfeatures.set_params(FlareTime2Peak=None)      #this doesn't seem to work, \n",
    "                                                                #need to figure out how to turn off features\n",
    "                                                                #shape of features should change\n",
    "\n",
    "#these are the keywords we want to tune in Logisitic REgression\n",
    "parameters = {\n",
    "    \"threshold\":[0.5],\n",
    "    \"estimator__max_iter\": [100],\n",
    "}\n",
    "\n",
    "model_tunning = GridSearchCV(model_to_set, param_grid=parameters)\n",
    "model_tunning.fit(X_train, y_train)\n",
    "\n",
    "print(model_tunning.best_score_)\n",
    "print(model_tunning.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
