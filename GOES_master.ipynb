{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module queries the GOES event catalog and makes a dataframe containing the flare onset, max, and end time as well as optlocation, active region, flare peak flux, and flare class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "from sunpy.instr.goes import get_goes_event_list\n",
    "from sunpy.time import parse_time, TimeRange, is_time_in_given_format\n",
    "from sunpy.instr.goes import flareclass_to_flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query GOES event catalog to find all flares between 2005 and 2017\n",
    "flare_2005 = datetime.datetime.strptime('2005-01-01T00:00:00.000', \"%Y-%m-%dT%H:%M:00.000\")\n",
    "flare_2017 = datetime.datetime.strptime('2018-01-01T00:00:00.000', \"%Y-%m-%dT%H:%M:00.000\")   \n",
    "\n",
    "t1_s = datetime.datetime.strftime(flare_2005, '%Y/%m/%d %H:%M')  \n",
    "t2_s = datetime.datetime.strftime(flare_2017, '%Y/%m/%d %H:%M')\n",
    "\n",
    "g_evt_list = get_goes_event_list(TimeRange(t1_s, t2_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new dataframe to hold new flares\n",
    "newFlr_df = pd.DataFrame(columns = ['FlrOnset','Flrmaxtime','Flrendtime','FlrPeakFlux','xrsclass','optlocation','region',\n",
    "                                   'TypeII','TypeIIDur','TypeIV','TypeIVDur','cmeonset','cmespeed','cmewidth','FlrIntFlux',\n",
    "                                   'FlrIntFlux2','tchianti','emchianti'])\n",
    "newFlr_df = newFlr_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import operator\n",
    "\n",
    "FlrOnset = []\n",
    "Flrmaxtime = []\n",
    "Flrendtime = []\n",
    "region = []\n",
    "FlrPeakFlux = []\n",
    "xrsclass = []\n",
    "optlocation = []\n",
    "\n",
    "flare_types = ['C','M','X']\n",
    "\n",
    "#looping through the GOES event list\n",
    "for i in range(len(g_evt_list)):\n",
    "    \n",
    "    #return the GOES flare class\n",
    "    g_class = g_evt_list[i]['goes_class']\n",
    "    max_flux = []\n",
    "    \n",
    "    #check if GOES class is C, M, or X\n",
    "    if g_class[0] in flare_types:\n",
    "        \n",
    "        #if yes then get time and location information\n",
    "        flare_start = datetime.datetime.strftime(g_evt_list[i]['start_time'], '%Y-%m-%dT%H:%M:00.000')\n",
    "        flare_peak = datetime.datetime.strftime(g_evt_list[i]['peak_time'], '%Y-%m-%dT%H:%M:00.000')\n",
    "        flare_end = datetime.datetime.strftime(g_evt_list[i]['end_time'], '%Y-%m-%dT%H:%M:00.000')\n",
    "        flare_loc = g_evt_list[i]['goes_location']\n",
    "        active_region = g_evt_list[i]['noaa_active_region']\n",
    "        max_flux.append(flareclass_to_flux(g_class))\n",
    "        \n",
    "        #try to find peak flux value for event. If val returns empty then add NaN because catalog does not always properly\n",
    "        #record flare class e.g. C?\n",
    "        try:\n",
    "            index, value = max(enumerate(max_flux), key=operator.itemgetter(1))\n",
    "            val = value.to_value(unit=None) #remove astropy units from flare peak flux\n",
    "        except:\n",
    "            val = np.nan\n",
    "        \n",
    "        FlrOnset.append(flare_start)\n",
    "        Flrmaxtime.append(flare_peak)\n",
    "        Flrendtime.append(flare_end)\n",
    "        region.append(active_region)\n",
    "        FlrPeakFlux.append(val)\n",
    "        xrsclass.append(g_class)\n",
    "        \n",
    "        #create appropriate strings to store flare location information\n",
    "        if flare_loc[0] >= 0:\n",
    "            EW_loc = 'W'\n",
    "        elif flare_loc[0] < 0:\n",
    "            EW_loc = 'E'\n",
    "        if flare_loc[1] >= 0:\n",
    "            NS_loc = 'N'\n",
    "        elif flare_loc[1] < 0:\n",
    "            NS_loc = 'S'\n",
    "        \n",
    "        #create the string e.g.N12E02 this format\n",
    "        flrLoc = NS_loc + str(abs(flare_loc[1])) + EW_loc + str(abs(flare_loc[0]))\n",
    "        optlocation.append(flrLoc)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#add to dataframe    \n",
    "newFlr_df['FlrOnset'] = FlrOnset\n",
    "newFlr_df['Flrmaxtime'] = Flrmaxtime\n",
    "newFlr_df['Flrendtime'] = Flrendtime\n",
    "newFlr_df['region'] = region\n",
    "newFlr_df['FlrPeakFlux'] = FlrPeakFlux\n",
    "newFlr_df['xrsclass'] = xrsclass\n",
    "newFlr_df['optlocation'] = optlocation    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "newFlr_df.to_csv('NewFlares_2005_2017.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now get GOES temperature and EM and integrated flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create datetime objects from date strings\n",
    "flare_max = df['Flrmaxtime']\n",
    "flare_start = df['FlrOnset']\n",
    "max_time = []\n",
    "start_time = []\n",
    "\n",
    "#convert flare max and flare start times to datetime objects\n",
    "for i in flare_max:\n",
    "    max_dt = datetime.datetime.strptime(i, \"%Y-%m-%dT%H:%M:%S.000\")\n",
    "    max_dt = pd.Timestamp(ts_input = max_dt)\n",
    "    max_time.append(max_dt)\n",
    "\n",
    "for j in flare_start:\n",
    "    start_dt = datetime.datetime.strptime(j, \"%Y-%m-%dT%H:%M:%S.000\")\n",
    "    start_dt = pd.Timestamp(ts_input = start_dt)\n",
    "    start_time.append(start_dt) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell gets temp,em, and integrated flux values for the flares. Because the code crashes so much due to the internet connection it saves files at different checkpoints. \n",
    "\n",
    "NOTE: some files contain data from previous files. This problem has not yet been inspected for the sake of time. Be sure that you are not concatanating files with repeated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read 1 minute GOES data\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from sunpy.lightcurve import LightCurve\n",
    "from sunpy.instr.goes import calculate_temperature_em\n",
    "\n",
    "#comment out after first run\n",
    "int_xs = [] #short xray\n",
    "int_xl = [] #long xray\n",
    "intflux_s = []\n",
    "intflux_l = []\n",
    "t_chianti = []\n",
    "em_chianti = []\n",
    "pkflux = []\n",
    "\n",
    "#points at which to save a new file\n",
    "checkpoints = [500,1000,1500,2000,2500,3000,3500,4000,4500,5000,5500,6000,6500,7000,7500,8000,8500,9000,9620]\n",
    "\n",
    "#choose points to check progress of codes\n",
    "check_prog = []\n",
    "\n",
    "for j in range(len(start_time)):\n",
    "    if j in check_prog:\n",
    "        print(j)\n",
    "    if isinstance(df['Flrmaxtime'][j],str) and isinstance(start_time[j],datetime.date): #check for nan and correct dates\n",
    "        flare_dat = df['Flrmaxtime'][j]\n",
    "        year = datetime.datetime.strptime(flare_dat,'%Y-%m-%dT%H:%M:00.000').strftime('%Y')\n",
    "        month = datetime.datetime.strptime(flare_dat,'%Y-%m-%dT%H:%M:00.000').strftime('%m')\n",
    "\n",
    "        monthstart = '01'\n",
    "        leap_years = np.arange(1984,2020,4)\n",
    "        \n",
    "        #date for end of the month\n",
    "        days_31 = ['01','03','05','07','08','10','12']\n",
    "        days_30 = ['04','06','09','11']\n",
    "\n",
    "        if (month in days_31) == True:\n",
    "            monthend = '31'\n",
    "\n",
    "        elif (month in days_30) == True:\n",
    "            monthend = '30'\n",
    "\n",
    "        elif month == '02':\n",
    "            if (year in leap_years) == True:\n",
    "                monthend = '29'\n",
    "            else:\n",
    "                monthend = '28'\n",
    "        \n",
    "        #access url where goes data is kept\n",
    "        temp_url = \"https://satdat.ngdc.noaa.gov/sem/goes/data/avg/\"+year+\"/\"+month+'/'\n",
    "        res = requests.get(temp_url)\n",
    "        soup = BeautifulSoup(res.content,'lxml')\n",
    "        table = soup.find_all('table')\n",
    "        df_ = pd.read_html(str(table))[1]\n",
    "        #goes_s =  list(df[0])[-1]\n",
    "        goes_s = list(df_.Name)[-2]\n",
    "        goes_num = goes_s[4:6]\n",
    "        path_to_csv = \"https://satdat.ngdc.noaa.gov/sem/goes/data/avg/\"+year+\"/\"+month+'/'+goes_s+\"csv/g\"+goes_num+\"_xrs_1m_\"+year+month+monthstart+\"_\"+year+month+monthend+\".csv\"\n",
    "\n",
    "        url = path_to_csv\n",
    "\n",
    "        import urllib\n",
    "        try: response = urllib.request.urlopen(url)\n",
    "        except: url = \"https://satdat.ngdc.noaa.gov/sem/goes/data/avg/\"+year+\"/\"+month+'/'+list(df_.Name)[-3]+\"csv/g\"+list(df_.Name)[-3][4:6]+\"_xrs_1m_\"+year+month+monthstart+\"_\"+year+month+monthend+\".csv\"\n",
    "        \n",
    "        try:\n",
    "            response = urllib.request.urlopen(url)\n",
    "\n",
    "            data = response.read().decode('utf-8')\n",
    "\n",
    "            datasplit = data.split('\\n')\n",
    "\n",
    "            #put data in readable format and looks for line where xray data begins\n",
    "            for rownum, row in enumerate(datasplit):\n",
    "                if row[0:8] == \"time_tag\":\n",
    "                    nskiprows = rownum + 1\n",
    "\n",
    "            #parsing function\n",
    "            parser = lambda x: pd.datetime.strptime(x, \"%Y-%m-%d %H:%M:%S.%f\")          \n",
    "\n",
    "            #read in cvs file from url and parse column time_tag with the function parse\n",
    "            if url[59:61] == '12':\n",
    "                c = pd.read_csv(url, skiprows=nskiprows, parse_dates=['time_tag'] ,date_parser = parser,names = ['time_tag','xs','xl'])\n",
    "            else:\n",
    "                c = pd.read_csv(url, skiprows=nskiprows, parse_dates=['time_tag'] ,date_parser = parser,names = ['time_tag','xs','xl'],usecols = [0,3,6])\n",
    "\n",
    "            #set time_tag to the index\n",
    "            c.set_index('time_tag',inplace=True)  \n",
    "\n",
    "            #find just the subset of dates between the flare start time and end time - has to be a date time format\n",
    "            t1_s = datetime.datetime.strftime(start_time[j], '%Y/%m/%d %H:%M')  \n",
    "            t2_s = datetime.datetime.strftime(end_time[j], '%Y/%m/%d %H:%M')\n",
    "            data_subset = c[t1_s:t2_s]  \n",
    "\n",
    "            #sum over the time - 60 second time intervals\n",
    "            sum_xrsa = (60 * data_subset['xs']).sum()\n",
    "            sum_xrsb = (60 * data_subset['xl']).sum()\n",
    "\n",
    "            int_xs.append(sum_xrsa)\n",
    "            int_xl.append(sum_xrsb)\n",
    "\n",
    "            try:\n",
    "                #integrated flux with background radiation subtracted\n",
    "                flx_s = (60 * (data_subset['xs'] - data_subset['xs'][0])).sum()\n",
    "                flx_l = (60 * (data_subset['xl'] - data_subset['xl'][0])).sum()\n",
    "            except:\n",
    "                flx_s = np.nan\n",
    "                flx_l = np.nan\n",
    "\n",
    "            intflux_s.append(flx_s)\n",
    "            intflux_l.append(flx_l)\n",
    "\n",
    "            ###temperature and emission measures\n",
    "            #convert the 1 min GOES data into a GOES lightcurve object - required for temp and em calculation\n",
    "            lc = LightCurve.create({\"xrsa\": data_subset['xs'], \"xrsb\":data_subset['xl']}, index = data_subset.index)\n",
    "\n",
    "            #add the satellite id to the meta data of the lightcurve object i.e. GOES 10, GOES 11, GOES 12\n",
    "            name1 = goes_s\n",
    "            name2 = name1.upper().split('S')\n",
    "            name3 = name2[0] + 'S ' + name2[1][0:2]\n",
    "            lc.meta[\"TELESCOP\"] = name3\n",
    "\n",
    "            #if an error arises and the temp is not in the accpetable range check to see if there are any \n",
    "            #negative values equal to -9999.0 in the array. keep only the rows that don't have an exray \n",
    "            #flux of -99999 (or whatever value they use that marks a bad value)\n",
    "            lc.data.xrsa = lc.data.xrsa[lc.data.xrsa != -99999.0]\n",
    "            lc.data.xrsb = lc.data.xrsb[lc.data.xrsb != -99999.0]\n",
    "\n",
    "            try:\n",
    "                #calculate the temperature and emission measure\n",
    "                lc_new = calculate_temperature_em(lc) \n",
    "\n",
    "                #flare max temp and em\n",
    "                temp = lc_new.data.temperature.max()\n",
    "                em = lc_new.data.em.max()\n",
    "\n",
    "                t_chianti.append(temp)\n",
    "                em_chianti.append(em)\n",
    "\n",
    "                #flare peak flux \n",
    "                flrpk = data_subset['xl'].max()\n",
    "                pkflux.append(flrpk)\n",
    "\n",
    "            except:\n",
    "                t_chianti.append(np.nan)\n",
    "                em_chianti.append(np.nan)\n",
    "                pkflux.append(flrpk)\n",
    "    \n",
    "        except:\n",
    "            int_xs.append(np.nan)\n",
    "            int_xl.append(np.nan)\n",
    "            intflux_s.append(np.nan)\n",
    "            intflux_l.append(np.nan)\n",
    "            t_chianti.append(np.nan)\n",
    "            em_chianti.append(np.nan)\n",
    "            pkflux.append(np.nan)\n",
    "    \n",
    "    else:\n",
    "        int_xs.append(np.nan)\n",
    "        int_xl.append(np.nan)\n",
    "        intflux_s.append(np.nan)\n",
    "        intflux_l.append(np.nan)\n",
    "        t_chianti.append(np.nan)\n",
    "        em_chianti.append(np.nan)\n",
    "        pkflux.append(np.nan)\n",
    "    \n",
    "    if j in checkpoints:\n",
    "        temp_df = pd.DataFrame(columns= ['IntFlux_s','IntFlux_l','Intbkg_s','Intbkg_l','tchianti','emchianti'])\n",
    "        temp_df = temp_df.fillna(np.nan)\n",
    "        temp_df['IntFlux_s'] = int_xs\n",
    "        temp_df['IntFlux_l'] = int_xl\n",
    "        temp_df['Intbkg_s'] = intflux_s\n",
    "        temp_df['Intbkg_l'] = intflux_l\n",
    "        temp_df['tchianti'] = t_chianti\n",
    "        temp_df['emchianti'] = em_chianti\n",
    "        \n",
    "        file_str = 'new_events_' + str(j) + 'n.csv'\n",
    "        temp_df.to_csv(file_str) #change number\n",
    "        print(j)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell needs to contain some code to bring together the incremental files to make final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #fixing indices\n",
    "# intfluxl = final_df['IntFlux_l'][:]\n",
    "# intfluxs = final_df['IntFlux_s'][:]\n",
    "# t_chianti = final_df['tchianti'][:]\n",
    "# em_chianti = final_df['emchianti'][:]\n",
    "\n",
    "# intfluxl.index = range(0,len(intfluxl))\n",
    "# intfluxs.index = range(0,len(intfluxs))\n",
    "# t_chianti.index = range(0,len(t_chianti))\n",
    "# em_chianti.index = range(0,len(em_chianti))\n",
    "\n",
    "# df['FlrIntFlux'] = intfluxl\n",
    "# df['FlrIntFlux2'] = intfluxs\n",
    "# df['tchianti'] = t_chianti\n",
    "# df['emchianti'] = em_chianti\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('new_events_df.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only run the next cell if you need to calculate the flare peak flux because it is very slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ####ONLY RUN THIS IF YOU NEED THE FLARE PEAK FLUX####\n",
    "\n",
    "# #from datetime import timedelta, datetime #, strptime, strftime\n",
    "# from sunpy.instr.goes import get_goes_event_list\n",
    "# from sunpy.time import parse_time, TimeRange, is_time_in_given_format\n",
    "# from sunpy.instr.goes import flareclass_to_flux\n",
    "# from sunpy.lightcurve import GOESLightCurve\n",
    "# from datetime import timedelta\n",
    "# import datetime\n",
    "# import time\n",
    "\n",
    "# flare_max = df['Flrmaxtime']\n",
    "\n",
    "# goes_class = []\n",
    "# start_time = []\n",
    "# end_time = []\n",
    "\n",
    "# x = -1\n",
    "# for g in range(len(flare_max)):\n",
    "#     time.sleep(0.1)\n",
    "#     if isinstance(g,str) == True: #make sure its not nan\n",
    "#         x = x+1\n",
    "#         error = 0 #to check whether or not we were able to match peak times (refer to nested for loop)\n",
    "#         flare_dt = datetime.datetime.strptime(g, \"%Y-%m-%dT%H:%M:00.000\") \n",
    "\n",
    "#         #convert to the format that needed for sunny TimeFrame function - add some time to the start and end times\n",
    "#         t1 = flare_dt - timedelta(seconds = (60*60*12))   \n",
    "#         t2 = flare_dt + timedelta(seconds = (60*60*12))   \n",
    "\n",
    "#         t1_s = datetime.datetime.strftime(t1, '%Y/%m/%d %H:%M')  \n",
    "#         t2_s = datetime.datetime.strftime(t2, '%Y/%m/%d %H:%M')\n",
    "\n",
    "#         #query the goes event list for all flares before and after the flare peak time of the event you are interested in\n",
    "#         g_evt_list = get_goes_event_list(TimeRange(t1_s, t2_s))\n",
    "\n",
    "#         for i in g_evt_list:\n",
    "#             if i['peak_time'] == flare_dt: #match peak times in our data and the goes catalog\n",
    "#                 gc = i['goes_class']\n",
    "#                 et = i['end_time']\n",
    "#                 st = i['start_time']\n",
    "#                 flx = flareclass_to_flux(gc)\n",
    "#                 goes_class.append(flx)\n",
    "#                 end_time.append(et)\n",
    "#                 start_time.append(st)\n",
    "#                 error = 1\n",
    "                \n",
    "#         if error == 0: #only if peak times didn't match\n",
    "#             goes_class.append(np.nan)\n",
    "#             end_time.append(np.nan)\n",
    "#             start_time.append(np.nan)\n",
    "        \n",
    "        \n",
    "        \n",
    "#     else:\n",
    "#         goes_class.append(np.nan)\n",
    "#         end_time.append(np.nan)\n",
    "#         start_time.append(np.nan)\n",
    "        \n",
    "# #sep_new['Importance'] = goes_class"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
