{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEP Forecasting - Logisitic Regression\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current physics based models of SEPs are unable to execute sufficiently fast in order to\n",
    "provide actionable information towards forecasting such disturbances, which can impact\n",
    "Earth within tens of minutes of the onset of an eruptive event. This is compounded by\n",
    "the intrinsic latency of certain key observations, which are used to define the initial\n",
    "conditions of these models. Instead, there is a reliance on statistical models to provide\n",
    "forecast probabilities of Earth-bound SEPs using real-time data. Since the largest, most\n",
    "impactful events occur infrequently, some regions of the feature space are sparse and\n",
    "simple discrete binning procedures have limitations. The goal of this project is to\n",
    "improve upon the empirical SEP proton prediction forecast model (PROTONS) currently\n",
    "in operational use at SWPC, through the application of modern machine learning\n",
    "techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "from datetime import datetime,timedelta\n",
    "\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, make_scorer, recall_score, matthews_corrcoef\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, GridSearchCV\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, brier_score_loss, precision_score\n",
    "from sklearn.metrics import brier_score_loss as bsl\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn import preprocessing\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "#import scikitplot as skplt\n",
    "\n",
    "\n",
    "from scipy.sparse import csr_matrix, find\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in and format the original Balch 2008 event list \n",
    "\n",
    "Choice of files: \n",
    "\n",
    "SPEall.v7p.xls - original SEP (only) event list from Balch 2008  \n",
    "ctrlevents.v8p.xls - original event list (SEP + control) from Balch 2008  \n",
    "ControlEvents_student.xls - Balch 2008 (SEP + control) event list with added CME speed and GOES T and EM from Dougs student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read in the original Balch SEP event list with added CME speed and GOES T and EM\n",
    "#orig_data = pd.read_excel(\"ControlEvents_student.xls\")\n",
    "\n",
    "#adding y label column indicating positive and negative SEP events - SEP events have Association = ProtonFlare\n",
    "#orig_data['sep'] = orig_data.Association.str.contains('^Proton').astype(int)\n",
    "\n",
    "#convert the type II and type IV association to binary \n",
    "#orig_data.TypeII = (orig_data.TypeII.str.lower() == \"yes\").astype(int)\n",
    "#orig_data.TypeIV = (orig_data.TypeIV.str.lower() == \"yes\").astype(int)\n",
    "\n",
    "# Remove rows where optlocation = nan\n",
    "#orig_data = orig_data[orig_data.optlocation.astype('str') != 'nan']\n",
    "\n",
    "#shuffle the events so they are not organized \n",
    "#orig_data = shuffle(orig_data)\n",
    "\n",
    "#save the shuffled dataframe -- commented out to prevent resaving\n",
    "#orig_data.to_csv(\"AllEvtsShuffled_1986_2004_cme.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the new SEP events from 2004 onwards\n",
    "\n",
    "Created by Pedro Brae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read in the SEP event list from 2004 - 2017\n",
    "#new_data = pd.read_csv('sep_events_2004_2017.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Balch Features\n",
    "\n",
    "From the original Balch 1999 and 2008 paper analysis only 4 features were used. GOES flare peak soft X-ray peak (1-8 A channel), flare soft X-ray integrated flux, occurrence of Type II and Type IV radio bursts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BalchPaperFeatures(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        \"\"\"Class to create original 4 features from Balch 2008\"\"\"\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def fit(self, examples):\n",
    "        # return self and nothing else \n",
    "        return self\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        \"\"\"Array mapping from feature integer indices to feature name\"\"\"\n",
    "        \n",
    "        return ['FlrPeakFlux','FlrIntFlux','TypeII','TypeIV']\n",
    "    \n",
    "    def transform(self, examples):\n",
    "                \n",
    "        #Choose the orginal 4 Balch 2008 features\n",
    "        X = examples[['FlrPeakFlux','FlrIntFlux2','TypeII','TypeIV']]\n",
    "        \n",
    "        return(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw features\n",
    "\n",
    "Other features read directly from the original event list i.e. the flare integrated soft X-ray flux at the GOES short wavelength channel (0.5 - 1 A), the GOES temperature and GOES emission measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RawFeatures(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        \"\"\"Other features from the Event List that don't need to be manipulated before being included in model\"\"\"\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def fit(self, examples):\n",
    "        # return self and nothing else \n",
    "        return self\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        \"\"\"Array mapping from feature integer indices to feature name\"\"\"\n",
    "        \n",
    "        return ['FlrIntFlux','tchianti','emchianti']\n",
    "    \n",
    "    def transform(self, examples):\n",
    "                \n",
    "        #Choose the orginal 4 Balch 2008 features (5 if you include both type II and type Iv as separate features)\n",
    "        X = examples[['FlrIntFlux','tchianti','emchianti']]\n",
    "\n",
    "        return(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flare time to peak\n",
    "\n",
    "The time between the flare onset and the flare peak time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FlareTime2Peak(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    \"\"\"Class to create feature with the time between flare onset to flare max time\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def fit(self, examples):\n",
    "        # return self and nothing else \n",
    "        return self\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        \"\"\"Array mapping from feature integer indices to feature name\"\"\"\n",
    "        \n",
    "        return [\"FlTim2Pk\"]|\n",
    "    \n",
    "    def transform(self, examples):\n",
    "        \n",
    "        # Initiaize matrix \n",
    "        X = np.zeros((len(examples),1))           \n",
    "\n",
    "        #time between flare max and flare onset\n",
    "        X[:,0] = np.asarray([(mx - on).seconds for mx, on in zip(examples.Flrmaxtime, examples.FlrOnset)])\n",
    "        \n",
    "        return(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flare location\n",
    "\n",
    "The flare location formatted from N16W12 format to North-South and East-West of disk center values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LocationFeatures(BaseEstimator, TransformerMixin):\n",
    "   \n",
    "    \"\"\"Class to create feature with the time between flare onset to flare max time\"\"\"\n",
    "   \n",
    "    def __init__(self):\n",
    "       \n",
    "        return None\n",
    "   \n",
    "    def fit(self, examples):\n",
    "        # return self and nothing else \n",
    "        return self\n",
    "   \n",
    "    def get_feature_names(self):\n",
    "        \"\"\"Array mapping from feature integer indices to feature name\"\"\"\n",
    "       \n",
    "        return ['Loc']\n",
    "   \n",
    "    def transform(self, examples):\n",
    "       \n",
    "        # Initiaize matrix \n",
    "        X = np.zeros((len(examples),2))  \n",
    "       \n",
    "        #time between flare max and flare onset\n",
    "        #X[:,0] = np.asarray([str(x)[1:3] for x in examples.optlocation])   #north - south\n",
    "        #X[:,1] = np.asarray([str(x)[4::] for x in examples.optlocation])   #north - south\n",
    "       \n",
    "        for i,loc in enumerate(examples.optlocation):\n",
    "            lat = str(loc)[1:3]\n",
    "            if str(loc)[0] == 's' or str(loc)[0] == 'S':\n",
    "                X[i,0] = -int(lat)\n",
    "            else:\n",
    "                X[i,0] = (lat)\n",
    "\n",
    "            #west-east\n",
    "            long = str(loc)[4::]\n",
    "            if str(loc)[3] == 'w' or str(loc)[3] == 'W':\n",
    "                X[i,1] = -int(long)\n",
    "            else:\n",
    "                X[i,1] = int(long)\n",
    "\n",
    "        return(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SEP Class \n",
    "\n",
    "With functions to build the training feature list from the above feature extractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SEPClass(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, estimator, folds = 6, threshold=0.5):        # <--- other keywords to be used by Feature Union go here\n",
    "        \n",
    "        \"\"\"Class to fit and train Logistic Regression algorithm for SEP forecasting\n",
    "        \n",
    "        Input keywords:\n",
    "        \n",
    "        folds:        Number of cross validation folds to use\n",
    "        threshold:    Decision Boundary threshold\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.estimator = estimator     #estimator to use for classification e.g. LogReg or SVM\n",
    "        self.folds = folds             #cross validation folds for estimator\n",
    "        self.threshold = threshold     #decision boundary threshold\n",
    "    \n",
    "        #Set up the Feature union to combine Feature creating classes\n",
    "        self.allmyfeatures = FeatureUnion([\n",
    "            (\"BalchFeat\", BalchPaperFeatures()),\n",
    "            (\"RawFeat\", RawFeatures()),\n",
    "            (\"LocFeatures\", LocationFeatures())#,\n",
    "            #(\"FlareTime2Peak\", FlareTime2Peak())   \n",
    "        ])\n",
    "    \n",
    "    def build_train_features(self, examples):\n",
    "        \"\"\"\n",
    "        Method to take in training text features and do further feature engineering \n",
    "        Most of the work in this homework will go here, or in similar functions  \n",
    "        :param examples: currently just a list of forum posts  \n",
    "        \"\"\"  \n",
    "        \n",
    "        ##convert columns of time from string to datetime -- MOVE TO SEPARATE CLEANING FUNCTION\n",
    "        #examples.FlrOnset = pd.Series(datetime.strptime(t, \"%Y-%m-%dT%H:%M:%S.%f\") for t in examples.FlrOnset)\n",
    "        #examples.Flrmaxtime = pd.Series(datetime.strptime(t, \"%Y-%m-%dT%H:%M:%S.%f\") for t in examples.Flrmaxtime)\n",
    "        #examples.Flrendtime = pd.Series(datetime.strptime(t, \"%Y-%m-%dT%H:%M:%S.%f\") for t in examples.Flrendtime)\n",
    "        #examples.FlrHpTime = pd.Series(datetime.strptime(t, \"%Y-%m-%dT%H:%M:%S.%f\") for t in examples.FlrHpTime)\n",
    "                \n",
    "        return self.allmyfeatures.fit_transform(examples)\n",
    "        #normalize data here?\n",
    "\n",
    "    def get_test_features(self, examples):\n",
    "        \"\"\"\n",
    "        Method to take in test text features and transform the same way as train features \n",
    "        :param examples: currently just a list of forum posts  \n",
    "        \"\"\"\n",
    "    \n",
    "        return self.allmyfeatures.transform(examples)\n",
    "\n",
    "    def show_topX(self, num=3):\n",
    "        \"\"\"\n",
    "        prints the top num features for the positive class and the \n",
    "        top 10 features for the negative class. \n",
    "        \"\"\"\n",
    "        feature_names = np.asarray([x.split(\"__\")[1] for x in self.allmyfeatures.get_feature_names()])\n",
    "        topX = np.argsort(self.estimator.coef_[0])[-num:]\n",
    "        bottomX = np.argsort(self.estimator.coef_[0])[:num]\n",
    "        \n",
    "        print(feature_names)\n",
    "        \n",
    "        print(\"\\nTop 3 features for Pos and Neg\\n-------------------------\")\n",
    "        for fn in np.arange(1,num):\n",
    "            print(\"Pos %i: %s %f\" % (fn, feature_names[topX[-fn]], self.estimator.coef_[0,topX[-fn]]))\n",
    "        for fn in np.arange(0,num-1):\n",
    "            print(\"Neg %i: %s %f\" % (fn, feature_names[bottomX[fn-1]], self.estimator.coef_[0,bottomX[fn-1]]))\n",
    "            \n",
    "      \n",
    "    def show_misclassified(self):     \n",
    "\n",
    "        \"\"\"\n",
    "        Method to show the misclassified examples i.e. False Positives and False negatives \n",
    "        \"\"\"\n",
    "        \n",
    "        #get all the feature names\n",
    "        #words = feat.allmyfeatures.get_feature_names() #####\n",
    "        words = self.allmyfeatures.get_feature_names() #####\n",
    "        \n",
    "        # False positives\n",
    "        print(\"\\nSome misclassified examples:\")\n",
    "        falsepos = np.where((self.train_pred != self.y_train) & (self.train_pred == 1))[0]   #all false pos example rows\n",
    "        print(\"\\nPredicted SEP but labeled AllClear (False Pos) \\n------------------------- \")\n",
    "\n",
    "        for i in range(len(falsepos[0:10])):         #loop through falsepos examples\n",
    "            weights_falsepos = []\n",
    "            x = find(feat.X_train[falsepos[i]])      #find which features are used for this example\n",
    "            for ii in x[1]:                          #from sparse matrix get column indices corresponding to features\n",
    "                weights_falsepos.append((words[ii].split('__')[1], self.estimator.coef_[0,ii]))      #get the word and weight\n",
    "\n",
    "            print(\"label: %i, prediction %i, Neg Prob: %f, Pos Prob: %f, Ex No.: %i,  example: %s \" % \\\n",
    "                (self.y_train[falsepos[i]], self.train_pred[falsepos[i]] , self.train_pred_prob[falsepos[i]][0], \\\n",
    "                     self.train_pred_prob[falsepos[i]][1], falsepos[i], self.clean_examples[falsepos[i]]))\n",
    "            for j in weights_falsepos:\n",
    "                print(j)\n",
    "                \n",
    "        # False Negatives\n",
    "        falseneg = np.where((self.train_pred != self.y_train) & (self.train_pred == 0))[0]\n",
    "        print(\"\\nPredicted AllClear but labeled SEP (False Neg) \\n-------------------------\")\n",
    "\n",
    "        for i in range(len(falseneg[0:10])):\n",
    "            weights_falseneg = []\n",
    "            x = find(feat.X_train[falseneg[r]])\n",
    "            for ii in x[1]:\n",
    "                weights_falseneg.append((words[ii].split('__')[1], self.estimator.coef_[0,ii])) \n",
    "\n",
    "            print(\"label: %i, prediction %i, Neg Prob: %f, Pos Prob: %f, Ex No.: %i, \\nexample: %s \" % \\\n",
    "                (self.y_train[falseneg[i]], self.train_pred[falseneg[i]], self.train_pred_prob[falseneg[i]][0], \\\n",
    "                 self.train_pred_prob[falseneg[i]][1], falseneg[i], self.clean_examples[falseneg[i]]))\n",
    "            for j in weights_falseneg:\n",
    "                print(j)\n",
    "        \n",
    "        print(\"\\n\\n\")\n",
    "        print(words)\n",
    "       \n",
    "    def show_report(self, X, y, y_pred, y_pred_prob):\n",
    "        \n",
    "        \"\"\"Method to show a report card of the model fit. Generalized to work for both the training and test sets\n",
    "            \n",
    "            X : features \n",
    "            y : labels\n",
    "            y_pred : predicted labels \n",
    "            y_pred_prob : predicted probabilities\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        #print(\"\\nTRAINING SET\\n-------------------------------------------------------------\\n\")\n",
    "        \n",
    "        # Add confusion Matrix\n",
    "        tn, fp, fn, tp=confusion_matrix(y, y_pred).ravel()\n",
    "        print(\"True Pos: %i, True Neg: %i, False Pos: %i,. False Neg: %i\\n\" % (tp,tn,fp,fn))\n",
    "\n",
    "        print(\"\\nCross Validation Metric Scores\\n-----------------------------------\\n\")\n",
    "        \n",
    "        #recall\n",
    "        metric1 = 'recall'\n",
    "        rec_score = cross_val_score(self.estimator, X, y, \\\n",
    "                                    cv=self.folds,scoring=metric1)\n",
    "        print(metric1)\n",
    "        print(rec_score,'\\n')\n",
    "        \n",
    "        #precision\n",
    "        metric2 = 'precision'\n",
    "        prec_score = cross_val_score(self.estimator, X, y, \\\n",
    "                                     cv=self.folds,scoring=metric2)\n",
    "        print(metric2)\n",
    "        print(prec_score,'\\n')\n",
    "    \n",
    "        #matthews correlation\n",
    "        metric3s = 'matthews correlation coefficient' \n",
    "        metric3 = make_scorer(matthews_corrcoef)\n",
    "        mattco_score = cross_val_score(self.estimator, X, y, \\\n",
    "                                       cv=self.folds,scoring=metric3)\n",
    "        print(metric3s)\n",
    "        print(mattco_score,'\\n')        \n",
    "    \n",
    "        #brier\n",
    "        metric4s = \"brier score\" \n",
    "        metric4 = make_scorer(brier_score_loss)\n",
    "        brier_score = cross_val_score(self.estimator, X, y, cv=self.folds,scoring=metric4)\n",
    "        print(metric4s)\n",
    "        print(brier_score,'\\n')        \n",
    "        \n",
    "\n",
    "        #cross validation\n",
    "        #print(\"\\nCross Validation Accuracy Scores (cross_val_predict)\\n-------------------------\")\n",
    "        #self.y_pred = cross_val_predict(self.estimator, self.X_train, self.y_train, cv=self.folds)\n",
    "        #print(self.score(self.y_pred, self.y_train))\n",
    "                \n",
    "        #training set score (no cross-validation)\n",
    "        print('\\nFull dataset (no cross-validation) Score \\n-----------------------------------\\n')\n",
    "        \n",
    "        rec1_score = recall_score(y,y_pred)\n",
    "        print(\"Recall: %.4f\" % rec1_score)\n",
    "        \n",
    "        prec1_score = precision_score(y,y_pred)\n",
    "        print(\"Precision: %.4f\" % prec1_score,'\\n')        \n",
    "        \n",
    "        mattco1_score = matthews_corrcoef(y,y_pred)\n",
    "        print(\"Mathews Corr: %.4f\" % mattco1_score,'\\n')        \n",
    "\n",
    "        #calculate the mean square error\n",
    "        mserr = mse(y, y_pred)\n",
    "        print(\"MSE: %.4f\" % mserr)\n",
    "\n",
    "        #calculate the Brier score - is this the same as the MSE? And QR referenced in Balch paper?\n",
    "        bsloss = bsl(y, y_pred_prob[:,1])\n",
    "        print(\"BSL: %.4f\" % bsloss)\n",
    "\n",
    "        #Occurance rate = #SEPS / #events\n",
    "        occ_rate = y.sum()/len(y)\n",
    "\n",
    "        #Reference score of predicting all negative class\n",
    "        #QR_star = 0.0324 Balch\n",
    "        QR_star =  mse(y, np.zeros(len(y)))\n",
    "        print(\"RefQuadScore: %.4f\\n\" % QR_star)\n",
    "\n",
    "        #assume the QR is the same as the MSE - as stated later in Balch\n",
    "        QR = mserr\n",
    "        #QR = 0.0250 Balch\n",
    "\n",
    "        #skill score \n",
    "        SS = (QR_star - QR)/QR_star\n",
    "        print(\"SS: %.4f\\n\" % SS)\n",
    "\n",
    "        FAR = fp/(tp + fp)\n",
    "        POD = tp/(tp + fn)\n",
    "        TSS = tp / (tp+fp+fn)\n",
    "        E = ((tp + fn)*(tp + fp) + (fp + tn)*(fn + tn)) / X.shape[0]\n",
    "        HSS = (tp + tn - E)/(X.shape[0] - E)\n",
    "\n",
    "        print(\"FAR: %.4f\" % FAR)\n",
    "        print(\"POD: %.4f\" % POD)\n",
    "        print(\"TSS: %.4f\" % TSS)\n",
    "        print(\"HSS: %.4f\" % HSS)\n",
    "        \n",
    "        ##ROC metrics\n",
    "        #fpr, tpr, thresh = roc_curve(self.y_train, self.y_train_pred, drop_intermediate=False)\n",
    "\n",
    "    def roc_curve(self):\n",
    "        \n",
    "        \"\"\"plot and ROC curve\"\"\"\n",
    "        \n",
    "        # Initial implementation of ROC plot applied to training set\n",
    "        skplt.metrics.plot_roc_curve(self.y_train, self.y_train_probas)\n",
    "        \n",
    "    def score(self, X, y):\n",
    "\n",
    "        \"\"\"find the accuracy score given the training data and labels\"\"\"\n",
    "        #print(\"...In Score...\")\n",
    "        #print(\"threshold:\", self.threshold)\n",
    "        \n",
    "        y_pred = (self.estimator.predict(X) > self.threshold).astype(int)\n",
    "        \n",
    "        #tn, fp, fn, tp=confusion_matrix(y, y_pred).ravel()\n",
    "        #return tp/(tp+fp+fn)\n",
    "        \n",
    "        return accuracy_score(y, y_pred)\n",
    "        \n",
    "    def fit(self, X, y):    #, random_state=1234):\n",
    "        \"\"\"\n",
    "        Method to read in training data from file, and \n",
    "        train Logistic Regression classifier. \n",
    "        \n",
    "        :param random_state: seed for random number generator \n",
    "        \"\"\"\n",
    "        \n",
    "        from sklearn.linear_model import LogisticRegression \n",
    "        \n",
    "        # load data \n",
    "        #self.dfTrain = pd.read_csv(\"AllEvtsShuffled.csv\")\n",
    "                      \n",
    "        # get training features and labels \n",
    "        #self.X_train = self.build_train_features(self.dfTrain)    #CHANGE\n",
    "        #self.y_train = np.array(self.dfTrain.sep, dtype=int)\n",
    "        \n",
    "        self.X_train = X \n",
    "        self.y_train = y \n",
    "        \n",
    "        #print the shape of the features\n",
    "        print(\"Shape of the Features: Num examples x Num Features\")\n",
    "        print(self.X_train.shape)\n",
    "        #print(\"examples...:\", self.X_train[0:10])\n",
    "\n",
    "        #self.logreg.fit(self.X_train, self.y_train)\n",
    "        self.estimator.fit(self.X_train, self.y_train)\n",
    "        \n",
    "        # make predictions on training data \n",
    "        self.y_train_pred = self.estimator.predict(self.X_train)\n",
    "\n",
    "        #return the LogReg probabilities used to classify each example  \n",
    "        self.y_train_pred_prob = self.estimator.predict_proba(self.X_train)\n",
    "        \n",
    "\n",
    "        #print report card\n",
    "        print(\"\\nTraining Set Report Card:\\n----------------------------------------\\n\")\n",
    "        print(sep.show_report(self.X_train, self.y_train, self.y_train_pred, self.y_train_pred_prob))\n",
    "        \n",
    "    \n",
    "    def predict(self, X, y):\n",
    "        \n",
    "        \"\"\"\n",
    "        Return predicted labels for exmaples X. \n",
    " \n",
    "        #### CURRENTLY THIS FUNCTION ISN'T USED - but could be called if we need predicted y vals \n",
    "        #### independently to the score function\n",
    " \n",
    "        \"\"\"\n",
    "        \n",
    "        self.X_test = X\n",
    "        self.y_test = y\n",
    "    \n",
    "        self.y_test_pred = self.estimator.predict(self.X_test)\n",
    "        self.y_test_pred_prob = self.estimator.predict_proba(self.X_test)\n",
    "        \n",
    "        print(\"\\nTest Set Report Card:\\n----------------------------------------\\n\")\n",
    "        print(sep.show_report(self.X_test, self.y_test, self.y_test_pred, self.y_test_pred_prob))\n",
    "        \n",
    "    def model_predict(self):\n",
    "        \"\"\"\n",
    "        Method to read in test data from file, make predictions\n",
    "        using trained model, and dump results to file \n",
    "        \n",
    "        #### CURRENTLY THIS FUNCTION ISN'T USED - leftover from FeatEngr homework but we might need to\n",
    "        #### to test on the holy grail test set\n",
    "        \"\"\"\n",
    "        \n",
    "        # read in test data \n",
    "        dfTest  = pd.read_csv(\"../data/spoilers/test.csv\")\n",
    "        \n",
    "        # featurize test data \n",
    "        self.X_test = self.get_test_features(list(dfTest[\"sentence\"]))\n",
    "        \n",
    "        # make predictions on test data \n",
    "        pred = self.estimator.predict(self.X_test)\n",
    "        \n",
    "        # dump predictions to file for submission to Kaggle  \n",
    "        #pd.DataFrame({\"spoiler\": np.array(pred, dtype=bool)}).to_csv(\"prediction.csv\", index=True, index_label=\"Id\")\n",
    "        \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize SEP object and create feature data\n",
    "\n",
    "Read in the shuffled data and pull out only the features that we need for analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the shuffled dataframe of original sep+ctrl events from 1986 - 2004\n",
    "orig_data = pd.read_csv(\"AllEvtsShuffled_1986_2004_cme.csv\")\n",
    "\n",
    "feature_data_old = orig_data[['FlrOnset','Flrmaxtime','FlrPeakFlux','FlrIntFlux2','TypeII','TypeIV','optlocation','tchianti','emchianti','FlrIntFlux','sep']]\n",
    "dfTrain = feature_data_old\n",
    "\n",
    "# to add in the SEP events from 2004 to 2017 uncomment this section\n",
    "#feature_data_new = new_data[['FlrOnset','Flrmaxtime','FlrPeakFlux','FlrIntFlux2','TypeII','TypeIV','optlocation','tchianti','emchianti','FlrIntFlux','sep']]\n",
    "#dfTrain = pd.concat([feature_data_old,feature_data_new])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialize SEPClass instance with an estimator of choice - here Logisitic Regression\n",
    "#with estimator specific keywords \n",
    "keywords = {'random_state':1230, 'max_iter':150}\n",
    "\n",
    "#sep_weights = {0:12,1:39} #dictionary for experimental class weights\n",
    "sep = SEPClass(LogisticRegression(**keywords, class_weight = 'balanced', \\\n",
    "                C=0.9,verbose=0,solver='newton-cg'),threshold = 0.5)\n",
    "#c,verbose,class_weight\n",
    "\n",
    "#Turn off features that aren't fully fleshed out or don't work\n",
    "#sep.allmyfeatures.set_params(RawFeat=None)\n",
    "\n",
    "# get training features and labels \n",
    "X_data = sep.build_train_features(dfTrain)    \n",
    "y_data = np.array(dfTrain.sep, dtype=int)\n",
    "\n",
    "#split dataset into training and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data,y_data,\\\n",
    "                            test_size=0.2, random_state=1230, stratify=y_data)\n",
    "\n",
    "#standardization\n",
    "X_train = preprocessing.scale(X_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypertune parameters with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logisitic regression object\n",
    "sep_lr = LogisticRegression(**keywords, C=0.9,verbose=0,solver='newton-cg')\n",
    "\n",
    "#set up scorers\n",
    "metric3 = make_scorer(matthews_corrcoef)\n",
    "metric4 = make_scorer(brier_score_loss)\n",
    "\n",
    "def tss_score(y, y_pred):\n",
    "    tn, fp, fn, tp=confusion_matrix(y, y_pred).ravel()\n",
    "    tss = tp / (tp + fp + fn)\n",
    "    return tss\n",
    "metric5 = make_scorer(tss_score)\n",
    "\n",
    "#hyperparameter ranges\n",
    "CV_dict = {'C':np.arange(0.01,0.11,0.01),'class_weight':[None,'balanced'],\\\n",
    "           'solver':['liblinear','newton-cg','lbfgs']}\n",
    "cv_search = GridSearchCV(estimator = sep_lr, param_grid = CV_dict, scoring = metric5)\n",
    "cv_search.fit(X_train,y_train)\n",
    "cv_result = cv_search.best_params_\n",
    "print(cv_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the tuned hyperparameters to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model\n",
    "keywords.update(cv_result)\n",
    "sep = SEPClass(LogisticRegression(**keywords),threshold = 0.5)\n",
    "sep.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set\n",
    "\n",
    "Now use the SEPClass predict method to make predictions on the test set and print out a report card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sep.predict(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration of model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FlrPeakFlux' 'FlrIntFlux' 'TypeII' 'TypeIV' 'FlrIntFlux' 'tchianti'\n",
      " 'emchianti' 'Loc']\n",
      "\n",
      "Top 3 features for Pos and Neg\n",
      "-------------------------\n",
      "Pos 1: emchianti 1.036395\n",
      "Pos 2: TypeIV 0.494658\n",
      "Neg 0: FlrPeakFlux -0.140693\n",
      "Neg 1: tchianti -0.595989\n"
     ]
    }
   ],
   "source": [
    "sep.show_topX(num=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "Xaxis = np.arange(-10., 10., 0.2)\n",
    "#Xaxis is an one-dimensional array with 100 elements\n",
    "#begin from -10.0 to 10.0, with interval of 0.2 for each two elements\n",
    "Yaxis = np.linspace(0, len(Xaxis), len(Xaxis))\n",
    "#Yaxis is also an one-dimensional arry with 100 elements\n",
    "#begin from 0 to 100\n",
    "\n",
    "plt.plot(Xaxis, Yaxis)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.patches as mpatches \n",
    "\n",
    "def sigmoid(x):\n",
    "    a = []\n",
    "    for item in x:\n",
    "        a.append(1/(1+math.exp(-item)))\n",
    "    return a\n",
    "\n",
    "x = np.arange(-10., 10., 0.2)\n",
    "sig = sigmoid(x)\n",
    "\n",
    "# Get current size\n",
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    " \n",
    "# Set figure width to 12 and height to 9\n",
    "fig_size[0] = 12\n",
    "fig_size[1] = 8\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "plt.plot(x,sig)\n",
    "plt.axhline(y=0.5, xmin=-10, xmax=10, hold=None,color = 'red')\n",
    "ax.set(title='Sigmoid Function')\n",
    "plt.xlabel('Feature Values',fontsize = 12)\n",
    "plt.ylabel('sig(z)',fontsize = 14)\n",
    "blue_patch = mpatches.Patch(color='blue', label='sig(z)')\n",
    "red_patch = mpatches.Patch(color='red', label='Threshold (sig(z) = 0.5)')\n",
    "plt.legend(handles=[blue_patch,red_patch])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
